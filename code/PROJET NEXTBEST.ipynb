{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "NEXTBEST",
      "provenance": [],
      "collapsed_sections": [
        "EnAdYqX8jHjf",
        "oJ5OOD15DYVK",
        "NARZbt1fUgMV",
        "XpYECGirpO8p",
        "B7FqB4eWjQIh",
        "i7BBms3pzU3A",
        "VtZE8CB7jv3e",
        "kpM2Sijsj1Ap",
        "viclgjIrzYSN",
        "L2_bgb25yk62",
        "B4mx2fsUrPPY",
        "_bvhNUPlMmlo",
        "bPacBQY86QaG",
        "emaFTvhB6UxY",
        "rIVf5px9KHBg",
        "8Gd8RLLH6fM6",
        "uwtfeH_R6iGV",
        "S9F3FN8NRc9O",
        "6wwgnDMsAtK1",
        "BcpPMmF7jDYb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C2sVllejPyT"
      },
      "source": [
        "# Group Project of Datamining & Machine learning, fall semester 2020 : **Group Tissot**\n",
        "*by Déborah Hernandez, François Grau and Simon Fellner*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0VxYM-E9ZFt"
      },
      "source": [
        "Here is the evolution of our acccuracy on AIcrowd :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTs-mzv-FcA"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnAdYqX8jHjf"
      },
      "source": [
        "# 1. Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2S7qS2QAjut"
      },
      "source": [
        "# Loading every module needed :\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re \n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets, tree\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from gensim.utils import simple_preprocess, lemmatize, deaccent\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import seaborn as sns\n",
        "\n",
        "# Fixing the SEED value for random_seed property :\n",
        "SEED = 72"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuwYb9IXGPg-"
      },
      "source": [
        "# Allows to speed up a bit\n",
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAWytV6bAx-S",
        "outputId": "a8a0364b-c7d8-4fdc-e240-805db79e9bc7"
      },
      "source": [
        "# Downloading the spacy module\n",
        "!python -m spacy download en\n",
        "\n",
        "# Import spacy \n",
        "import spacy\n",
        "\n",
        "# Lorading english dictionnary\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ5OOD15DYVK"
      },
      "source": [
        "#2. Acquisition des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2xRGiS2Ayg1",
        "outputId": "c6ed8906-a5ae-465d-b291-6194c7524cef"
      },
      "source": [
        "# Reading the data transfered on our github :\n",
        "Train = pd.read_csv('https://raw.githubusercontent.com/s-fellner/DMML2020_Tissot/main/data/Train_set.csv')\n",
        "Test = pd.read_csv('https://raw.githubusercontent.com/s-fellner/DMML2020_Tissot/main/data/Test_set.csv')\n",
        "\n",
        "# Replacing NaN values in the 'keyword' column :\n",
        "print('NaN values in original sets :\\n\\n', '- Train dataset :\\n',\n",
        "      Train.isnull().sum(), '\\n\\n- Test dataset :\\n', Test.isnull().sum(), \n",
        "      '\\n\\n-------------------------------------\\n')\n",
        "Test['keyword'].fillna('', inplace=True)\n",
        "Train['keyword'].fillna('', inplace=True)\n",
        "print('NaN values after replacement for \"keyword\" :\\n\\n', '- Train dataset :\\n',\n",
        "      Train.isnull().sum(), '\\n\\n- Test dataset :\\n', Test.isnull().sum())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaN values in original sets :\n",
            "\n",
            " - Train dataset :\n",
            " id             0\n",
            "keyword       55\n",
            "location    2141\n",
            "text           0\n",
            "target         0\n",
            "dtype: int64 \n",
            "\n",
            "- Test dataset :\n",
            " id            0\n",
            "keyword       6\n",
            "location    392\n",
            "text          0\n",
            "dtype: int64 \n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "NaN values after replacement for \"keyword\" :\n",
            "\n",
            " - Train dataset :\n",
            " id             0\n",
            "keyword        0\n",
            "location    2141\n",
            "text           0\n",
            "target         0\n",
            "dtype: int64 \n",
            "\n",
            "- Test dataset :\n",
            " id            0\n",
            "keyword       0\n",
            "location    392\n",
            "text          0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2IeW6NlEcWw",
        "outputId": "60d7ee56-6b3e-4bda-9abc-687aeae14a26"
      },
      "source": [
        "# Building the dataset of features X (building of the model) and X_fin (predictions) by adding the keyword at the begining of the text :\n",
        "X = Train['keyword'] + ' '  + Train['text']\n",
        "X_fin = Test['keyword'] + ' ' + Test['text']\n",
        "\n",
        "# Building the target dataset (building of the model)\n",
        "y = Train['target'].astype('int')\n",
        "\n",
        "X_fin"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       tsunami Crptotech tsunami and banks.\\n http://...\n",
              "1       traumatised I'm that traumatised that I can't ...\n",
              "2       burning%20buildings @foxnewsvideo @AIIAmerican...\n",
              "3       desolate Me watching Law &amp; Order (IB: @sau...\n",
              "4               crushed Papi absolutely crushed that ball\n",
              "                              ...                        \n",
              "1137    derailed @ItsQueenBaby I'm at work it's a bunc...\n",
              "1138    suicide%20bomber #?? #?? #??? #??? Suicide bom...\n",
              "1139    volcano Eruption of Indonesian volcano sparks ...\n",
              "1140    fear Never let fear get in the way of achievin...\n",
              "1141    refugees wowo--=== 12000 Nigerian refugees rep...\n",
              "Length: 1142, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpYECGirpO8p"
      },
      "source": [
        "# 3. Text Cleaning\n",
        "\n",
        "We tried to push the text cleaning really far by replacing special character, urls, mentions, hashtags, emojis and slang words. This task was completely manual (looking before / after cleaning and then generated tokens). But we observe that replacing slangs and hashtags reduced our accuracy (deleting # symbol or replacing #Word).\n",
        "\n",
        "Improving accuracy by replacing :\n",
        "\n",
        "*   'http\\xxxxxxxx' by 'website'\n",
        "*   @xxxx by 'user'\n",
        "*   deleting digits\n",
        "*   replacing emojis by the feeling associated to them (for example ':)' by 'happy')\n",
        "*   deleting all special characters (for example '&' + 'amp;' is the encoding of '&')\n",
        "\n",
        "Decreasing accuracy by replacing :\n",
        "\n",
        "*   '#xxxxxx' by 'hashtag' or deleting only '#' or completely deleting it\n",
        "*   slang words by their meaning (for example 'w/' by 'with')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej9vNzxepRSx"
      },
      "source": [
        "# Replacing every special characters, urls, mentions, emojis of the feature (for X and X_fin) :\n",
        "def cleaning(tweet):\n",
        "  tweet = tweet.lower()\n",
        "\n",
        "  tweet = re.sub(r'http\\S+', ' website', tweet)\n",
        "  tweet = re.sub(r'\\S+\\.com\\S+', ' website', tweet)\n",
        "  tweet = re.sub(r'\\S+\\.com\\b', ' website', tweet)\n",
        "  tweet = re.sub(r'@\\S+', 'user', tweet)\n",
        "  tweet = re.sub(r'@', ' ', tweet)\n",
        "\n",
        "  tweet = re.sub(r'%20', ' ', tweet)\n",
        "  tweet = re.sub(r'\\x89ûï', '',tweet)  \n",
        "  tweet = re.sub(r'\\x89ûª', \"'\",tweet)\n",
        "  tweet = re.sub(r'\\x89û', '',tweet)\n",
        "  tweet = re.sub(r'åê', ' ',tweet)\n",
        "  # tweet = re.sub(r'w/','with ',tweet)\n",
        "  tweet = re.sub(r'\\x9d', ' ',tweet)\n",
        "  #tweet = re.sub(r'#', 'hashtag ',tweet)\n",
        "  tweet = re.sub(r'Ûª', \"'\",tweet)\n",
        "  tweet = re.sub(r'\\.\\.\\.', ' ', tweet)\n",
        "  tweet = re.sub(r'\\.\\.', ' ', tweet)\n",
        "  tweet = re.sub(r'\\S+\\Û_', ' ', tweet)\n",
        "  tweet = re.sub(r'\\brt\\b', ' ', tweet)\n",
        "\n",
        "  tweet = re.sub(r'-', ' ', tweet)\n",
        "\n",
        "  tweet = re.sub('\\d+', '', tweet)\n",
        "  tweet = re.sub(r'&amp;', ' and ', tweet)\n",
        "  tweet = re.sub(r'&lt;', 'and', tweet)\n",
        "  \n",
        "  tweet = re.sub(r'\\s\\:p', ' funny ', tweet)\n",
        "  tweet = re.sub(r'\\s\\:\\)', ' happy ', tweet)\n",
        "  tweet = re.sub(r'\\s\\(\\:', ' happy ', tweet)\n",
        "  tweet = re.sub(r'\\s\\:s', ' annoyed ', tweet)\n",
        "  tweet = re.sub(r'\\s\\:\\(', ' sad ', tweet)\n",
        "  tweet = re.sub(r'\\s\\)\\:', ' sad ', tweet)\n",
        "  tweet = re.sub(r'\\s\\/:', ' annoyed ', tweet)\n",
        "  tweet = re.sub(r'\\sxd', ' funny ', tweet)\n",
        "  tweet = re.sub(r'\\s=p', ' funny ', tweet)\n",
        "  tweet = re.sub(r'\\s\\:\\/', ' annoyed ', tweet)\n",
        "  tweet = re.sub(r'\\s\\:o', ' surprised ', tweet)\n",
        "  tweet = re.sub(r'\\so\\:', ' surprised ', tweet)\n",
        "  tweet = re.sub(r'\\sd\\:', ' surprised ', tweet)\n",
        "\n",
        "  tweet = re.sub(r'\\x89û÷', '', tweet)\n",
        "  tweet = re.sub(r'beû_', '', tweet)\n",
        "  tweet = re.sub(r'\\x89û_', '', tweet)\n",
        "  tweet = re.sub(r'&gt', '', tweet)\n",
        "  tweet = re.sub(r'\\x89ûó', '', tweet)\n",
        "  tweet = re.sub(r'\\x89ûò', '', tweet)\n",
        "  tweet = re.sub(r'\\n', ' ', tweet)\n",
        "  \n",
        "  tweet = re.sub(r'÷', ' ', tweet)\n",
        "  tweet = re.sub(r'ó', ' ', tweet)\n",
        "  tweet = re.sub(r'ìü', ' ', tweet)\n",
        "  tweet = re.sub(r'ìñ', ' ', tweet)\n",
        "  tweet = re.sub(r'ì_', ' ', tweet)\n",
        "  tweet = re.sub(r'_', ' ', tweet)\n",
        "\n",
        "  # Old replacements : After test, those are decreasing accuracy \n",
        "  \n",
        "  # tweet = re.sub(r'\\byr\\b', 'year', tweet)\n",
        "  # tweet = re.sub(r'\\bur\\b', 'your', tweet)\n",
        "  # tweet = re.sub(r'\\blol\\b', 'funny', tweet)\n",
        "  # tweet = re.sub(r'\\bomg\\b', 'shocking', tweet)\n",
        "  # tweet = re.sub(r'\\bco\\.', 'company', tweet)\n",
        "  # tweet = re.sub(r\"x'mas\", 'christmas', tweet)\n",
        "  # tweet = re.sub(r'\\bvid\\b', 'video', tweet)\n",
        "  # tweet = re.sub(r\"\\bs'thing\\b\", 'something', tweet)\n",
        "  # tweet = re.sub(r\"\\both\\b\", 'over the hill', tweet)\n",
        "  # tweet = re.sub(r\"\\bde\\b\", 'of', tweet)\n",
        "  # tweet = re.sub(r\"\\bnite\\b\", 'night', tweet)\n",
        "  # tweet = re.sub(r\"\\brec\\b\", 'record', tweet)\n",
        "  # tweet = re.sub(r\"\\bidc\\b\", \"i don't care\", tweet)\n",
        "  # tweet = re.sub(r\"\\bthx\\b\", \"thanks\", tweet)\n",
        "  # tweet = re.sub(r\"\\bmfs\\b\", 'insult', tweet)\n",
        "  # tweet = re.sub(r\"\\bwtf\\b\", 'astonished', tweet)\n",
        "  # tweet = re.sub(r\"\\bslsp\\b\", 'something', tweet)\n",
        "  # tweet = re.sub(r\"\\bu\\.s\\.\", 'united states', tweet)\n",
        "  # tweet = re.sub(r\"\\ba\\.m\\.\", 'morning', tweet)\n",
        "  # tweet = re.sub(r\"\\bam\\b\", 'morning', tweet)\n",
        "  # tweet = re.sub(r\"\\bpm\\b\", 'afternoon', tweet)\n",
        "  # tweet = re.sub(r\"\\bokay\\b\", 'ok', tweet)\n",
        "  # tweet = re.sub(r\"\\betc\\.\", 'and so forth', tweet)\n",
        "  # tweet = re.sub(r\"\\bdept\\.\", 'department', tweet)\n",
        "  # tweet = re.sub(r\"\\bwut\\b\", 'what', tweet)\n",
        "  # tweet = re.sub(r\"\\bpls\\b\", 'please', tweet)\n",
        "  # tweet = re.sub(r\"\\blmao\\b\", 'funny', tweet)\n",
        "  # tweet = re.sub(r\"\\bhaha\\b\", 'funny', tweet)\n",
        "  # tweet = re.sub(r\"\\benroute\\b\", 'on the way', tweet)\n",
        "  # tweet = re.sub(r\"\\bgov\\.\", 'government', tweet)\n",
        "  # tweet = re.sub(r\"\\bdr\\.\", 'doctor', tweet)\n",
        "  # tweet = re.sub(r\"\\bkindof\\b\", 'kind of', tweet)\n",
        "  # tweet = re.sub(r\"\\bppl\\b\", 'people', tweet)\n",
        "  # tweet = re.sub(r\"\\bu\\b\", 'you', tweet)\n",
        "\n",
        "  return tweet"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRsbs02wWkhf",
        "outputId": "6104d522-f4dc-45c5-dabc-a9b8c7443397"
      },
      "source": [
        "# Random seed for samples\n",
        "sss = 57\n",
        "# Tweets before cleaning\n",
        "a = X.sample(20, random_state=sss).values\n",
        "print(a)\n",
        "print('-----------------------------------------------')\n",
        "\n",
        "# Cleaning the tweets\n",
        "X = X.apply(lambda x : cleaning(x))\n",
        "X_fin = X_fin.apply(lambda x : cleaning(x))\n",
        "\n",
        "# Tweets after cleaning\n",
        "a = X.sample(20, random_state=sss).values\n",
        "print(a)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"eyewitness @kaputt21 Hamburg Police Chief Gregory Wickett has told 7 Eyewitness News he 'can't confirm or deny' an investigation is underway.\"\n",
            " 'wild%20fires @randerson62 Watching news of wild fires and hope all is ok.'\n",
            " 'arson Arson suspect linked to 30 fires caught in Northern California http://t.co/u1fuWrGK5U'\n",
            " 'blizzard @StevenOnTwatter @PussyxDestroyer just order a blizzard pay then put your nuts in it say they have you ball flavored. Boom free ice cream'\n",
            " \"screaming MY FAVS I'M SCREAMING SO FUCKING LOUD http://t.co/cP7c1cH0ZU\"\n",
            " ' Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '\n",
            " \"injured @chikislizeth08 you're not injured anymore? ??\"\n",
            " \"quarantined Reddit's new content policy goes into effect many horrible subreddits banned or quarantined http://t.co/4oNvxncz8w http://t.co/tnggXNm6k8\"\n",
            " \"bioterrorism @O_Magazine satan's daughter shadow warrior in 50ft women aka transgender mode ps nyc is about to fold extra extra center of bioterrorism\"\n",
            " 'injured 4 dead dozens injured in Gaza blast near house leveled in summer war - http://t.co/L53OABEqc9 via http://t.co/q3Izqdk1n0'\n",
            " 'deluge @joshsternberg My feed seems to have a deluge once or twice during the week. It\\x89Ûªs fantastic.'\n",
            " 'obliterated Obliterated my phone screen today with a drum stick. #blessed'\n",
            " 'rioting I think Twitter was invented to keep us insomniacs from rioting in the wee small hours.'\n",
            " \"bridge%20collapse US wont upgrade its infrastructure? http://t.co/NGEHhG9YGa' it a bad situation and its going to get ugly very quickly #USA #sustainability\"\n",
            " 'bleeding Eating takis then rubbing my eyes with my hands now my eyes are bleeding tears'\n",
            " 'wounded @wocowae Officer Wounded Suspect Killed in Exchange of Gunfire http://t.co/QI2BDvkab7 ushed'\n",
            " 'fire Fire waves and darkness'\n",
            " 'refugees Captain Abbott must go down with LNP boat #refugees #christianvalues https://t.co/Kp5dpOaF58'\n",
            " 'bombing Japan Marks 70th Anniversary of Hiroshima Atomic Bombing http://t.co/jzgxwRgFQg'\n",
            " 'floods @casewrites when it rains in NJ it flash floods. Otherwise its just a desert of grief and taxes.']\n",
            "-----------------------------------------------\n",
            "[\"eyewitness user hamburg police chief gregory wickett has told  eyewitness news he 'can't confirm or deny' an investigation is underway.\"\n",
            " 'wild fires user watching news of wild fires and hope all is ok.'\n",
            " 'arson arson suspect linked to  fires caught in northern california  website'\n",
            " 'blizzard user user just order a blizzard pay then put your nuts in it say they have you ball flavored. boom free ice cream'\n",
            " \"screaming my favs i'm screaming so fucking loud  website\"\n",
            " ' just got sent this photo from ruby #alaska as smoke from #wildfires pours into a school '\n",
            " \"injured user you're not injured anymore? ??\"\n",
            " \"quarantined reddit's new content policy goes into effect many horrible subreddits banned or quarantined  website  website\"\n",
            " \"bioterrorism user satan's daughter shadow warrior in ft women aka transgender mode ps nyc is about to fold extra extra center of bioterrorism\"\n",
            " 'injured  dead dozens injured in gaza blast near house leveled in summer war    website via  website'\n",
            " \"deluge user my feed seems to have a deluge once or twice during the week. it's fantastic.\"\n",
            " 'obliterated obliterated my phone screen today with a drum stick. #blessed'\n",
            " 'rioting i think twitter was invented to keep us insomniacs from rioting in the wee small hours.'\n",
            " 'bridge collapse us wont upgrade its infrastructure?  website it a bad situation and its going to get ugly very quickly #usa #sustainability'\n",
            " 'bleeding eating takis then rubbing my eyes with my hands now my eyes are bleeding tears'\n",
            " 'wounded user officer wounded suspect killed in exchange of gunfire  website ushed'\n",
            " 'fire fire waves and darkness'\n",
            " 'refugees captain abbott must go down with lnp boat #refugees #christianvalues  website'\n",
            " 'bombing japan marks th anniversary of hiroshima atomic bombing  website'\n",
            " 'floods user when it rains in nj it flash floods. otherwise its just a desert of grief and taxes.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARZbt1fUgMV"
      },
      "source": [
        "# 4. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxMaLGiHwh3k"
      },
      "source": [
        "## 4.1. Base Rate\n",
        "We tried to rebalance the Train dataset by deleting entries to rebalance the br to 0.5, but then the model is less efficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPEe4-tMBAgU",
        "outputId": "ecb42888-30ea-4425-88e3-cb67121f5466"
      },
      "source": [
        "# Calculing the base rate of the dataset :\n",
        "def br_calc(target_col):\n",
        "  classes = {}\n",
        "  for target in target_col:\n",
        "    if target not in classes:\n",
        "      classes[target] = 1\n",
        "    else:\n",
        "      classes[target] += 1\n",
        "  br = max(classes.values())/sum(classes.values())\n",
        "  return br\n",
        "\n",
        "print('The base rate for target is', round(br_calc(Train['target']), 2))\n",
        "\n",
        "# # Create balanced dataframe - base rate = 0.5\n",
        "# Train_bal = pd.concat([Train[Train[\"target\"] == 0].sample(len(Train[Train[\"target\"] == 1])), Train[Train[\"target\"] == 1]], axis=0).reset_index()\n",
        "# Train = Train_bal\n",
        "# print(Train)\n",
        "\n",
        "# print('The base rate for target is', round(br_calc(Train['target']), 2))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The base rate for target is 0.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNWQNfiCqa3p"
      },
      "source": [
        "## 4.2. Adding features to datas\n",
        "For EDA and maybe improving model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh5BG4o9UiOi"
      },
      "source": [
        "# Counting specificities of a sentence\n",
        "def counterize(df):\n",
        "\n",
        "  word = []\n",
        "  mention = []\n",
        "  hashtag = []\n",
        "  capital = []\n",
        "  question = []\n",
        "  url = []\n",
        "  emoji = []\n",
        "  emojis_list = [\n",
        "  r'\\s\\:\\)',\n",
        "  r'\\s\\:\\(',\n",
        "  r'\\s\\(\\:',\n",
        "  r'\\s\\)\\:',\n",
        "  r'\\s\\:\\/',\n",
        "  r'\\s\\:D\\b',\n",
        "  r'\\sO\\:',\n",
        "  r'\\sD\\:',\n",
        "  r'\\so\\:',\n",
        "  r'\\s\\:O\\b',\n",
        "  r'\\s\\:o\\b',\n",
        "  r'\\s\\:\\|',\n",
        "  r'\\s\\:p\\b',\n",
        "  r'\\s\\:P\\b',\n",
        "  r'\\sxD\\b',\n",
        "  r'\\sx\\)',\n",
        "  r'\\sx\\(',\n",
        "  r'\\s\\(x\\b',\n",
        "  r'\\s=P\\b',\n",
        "  r'\\sXD\\b',\n",
        "  r'\\sxd\\b'        \n",
        "  ]\n",
        "\n",
        "  x=0\n",
        "  while x < df.shape[0]:\n",
        "    word.append(len(re.findall(r'\\w+', df['text'][x])))\n",
        "    mention.append(len(re.findall(r'@\\w+', df['text'][x])))\n",
        "    hashtag.append(len(re.findall(r'#\\w+', df['text'][x])))\n",
        "    capital.append(len(re.findall(r'\\b[A-Z]', df['text'][x])))\n",
        "    question.append(len(re.findall(r'!|\\?', df['text'][x])))\n",
        "    url.append(len(re.findall(r'http.?://[^\\s]+[\\s]?', df['text'][x])))\n",
        "    mogs = 0\n",
        "    for mog in emojis_list:\n",
        "      mogs += len(re.findall(mog, df['text'][x]))\n",
        "    emoji.append(mogs)\n",
        "    x+=1\n",
        "\n",
        "  df['word'] = word\n",
        "  df['mention'] = mention\n",
        "  df['hashtag'] = hashtag\n",
        "  df['capital'] = capital\n",
        "  df['question'] = question\n",
        "  df['url'] = url\n",
        "  df['emoji'] = emoji"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "sNpPpsqbUjtW",
        "outputId": "31f526cf-b901-43f4-e9bc-f06faebe4a01"
      },
      "source": [
        "# Adding features to Train and Test\n",
        "counterize(Train)\n",
        "counterize(Test)\n",
        "\n",
        "Train"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>word</th>\n",
              "      <th>mention</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>capital</th>\n",
              "      <th>question</th>\n",
              "      <th>url</th>\n",
              "      <th>emoji</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6466</th>\n",
              "      <td>4377</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>ARGENTINA</td>\n",
              "      <td>#Earthquake #Sismo M 1.9 - 15km E of Anchorage...</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6467</th>\n",
              "      <td>3408</td>\n",
              "      <td>derail</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@EmiiliexIrwin Totally agree.She is 23 and kno...</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6468</th>\n",
              "      <td>9794</td>\n",
              "      <td>trapped</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hollywood Movie About Trapped Miners Released ...</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6469</th>\n",
              "      <td>10344</td>\n",
              "      <td>weapons</td>\n",
              "      <td>Beirut/Toronto</td>\n",
              "      <td>Friendly reminder that the only country to eve...</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6470</th>\n",
              "      <td>1779</td>\n",
              "      <td>buildings%20on%20fire</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Buildings are on fire and they have time for a...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6471 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                keyword               location  ... question  url  emoji\n",
              "0      3738              destroyed                    USA  ...        0    0      0\n",
              "1       853              bioterror                    NaN  ...        0    1      0\n",
              "2     10540              windstorm  Palm Beach County, FL  ...        0    1      0\n",
              "3      5988              hazardous                    USA  ...        0    0      0\n",
              "4      6328                hostage             Australia   ...        0    1      0\n",
              "...     ...                    ...                    ...  ...      ...  ...    ...\n",
              "6466   4377             earthquake              ARGENTINA  ...        0    1      0\n",
              "6467   3408                 derail                    NaN  ...        0    0      0\n",
              "6468   9794                trapped                    NaN  ...        0    1      0\n",
              "6469  10344                weapons         Beirut/Toronto  ...        0    1      0\n",
              "6470   1779  buildings%20on%20fire                    NaN  ...        0    0      0\n",
              "\n",
              "[6471 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2lcN-3shyfN"
      },
      "source": [
        "# Keeping dataframe with target = 0 and target = 1\n",
        "Train_0 = Train[Train['target'] == 0]\n",
        "Train_1 = Train[Train['target'] == 1]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t-nIDI43Hyp8",
        "outputId": "4d05c71a-6695-44d3-de51-dc4f7a449961"
      },
      "source": [
        "# Boxplotting additional features per class\r\n",
        "fig, ax = plt.subplots(3,3, figsize=(30,20))\r\n",
        "ax[0,0].boxplot([Train_0['word'], Train_1['word']])\r\n",
        "ax[0,0].set_title('Number of words')\r\n",
        "ax[0,0].set_xticklabels([0,1], fontsize=12)\r\n",
        "ax[0,0].set_xlabel('Target')\r\n",
        "\r\n",
        "ax[0,1].boxplot([Train_0['mention'], Train_1['mention']])\r\n",
        "ax[0,1].set_title('Number of mention')\r\n",
        "ax[0,1].set_xticklabels([0,1], fontsize=12)\r\n",
        "ax[0,1].set_xlabel('Target')\r\n",
        "\r\n",
        "ax[0,2].boxplot([Train_0['hashtag'], Train_1['hashtag']])\r\n",
        "ax[0,2].set_title('Number of hashtag')\r\n",
        "ax[0,2].set_xticklabels([0,1], fontsize=12)\r\n",
        "ax[0,2].set_xlabel('Target')\r\n",
        "\r\n",
        "ax[1,0].boxplot([Train_0['capital'], Train_1['capital']])\r\n",
        "ax[1,0].set_title('Number of capital letter')\r\n",
        "ax[1,0].set_xticklabels([0,1], fontsize=12)\r\n",
        "ax[1,0].set_xlabel('Target')\r\n",
        "\r\n",
        "ax[1,1].boxplot([Train_0['question'], Train_1['question']])\r\n",
        "ax[1,1].set_title('Number of question/exclamation mark')\r\n",
        "ax[1,1].set_xticklabels([0,1], fontsize=12)\r\n",
        "ax[1,1].set_xlabel('Target')\r\n",
        "\r\n",
        "ax[1,2].boxplot([Train_0['url'], Train_1['url']])\r\n",
        "ax[1,2].set_title('Number of url')\r\n",
        "ax[1,2].set_xticklabels([0,1], fontsize=12)\r\n",
        "ax[1,2].set_xlabel('Target')\r\n",
        "\r\n",
        "ax[2,0].boxplot([Train_0['emoji'], Train_1['emoji']])\r\n",
        "ax[2,0].set_title('Number of emoji')\r\n",
        "ax[2,0].set_xticklabels([0,1], fontsize=12)\r\n",
        "ax[2,0].set_xlabel('Target')\r\n",
        "\r\n",
        "fig.delaxes(ax[2,1])\r\n",
        "fig.delaxes(ax[2,2])\r\n",
        "plt.draw()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x1440 with 7 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2al45yHccLp"
      },
      "source": [
        "We can deduce that the additionnal features 'hashtag', 'mention' and 'question' are interesting to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "dz2KXmNZcKy1",
        "outputId": "a14991e8-40c0-42e0-d80a-bba35dfdffb2"
      },
      "source": [
        "# Saving those feature in specifics variables\r\n",
        "Train_add = Train[['mention', 'hashtag', 'question']]\r\n",
        "Test_add = Test[['mention','hashtag', 'question']]\r\n",
        "\r\n",
        "# Setting scaler\r\n",
        "scaler = preprocessing.StandardScaler()\r\n",
        "\r\n",
        "# Normalise columns\r\n",
        "Train_add = pd.DataFrame(scaler.fit_transform(Train_add))\r\n",
        "Test_add = pd.DataFrame(scaler.transform(Test_add))\r\n",
        "\r\n",
        "Test_add"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.504068</td>\n",
              "      <td>3.256630</td>\n",
              "      <td>-0.295690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.504068</td>\n",
              "      <td>-0.399768</td>\n",
              "      <td>0.741952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.654653</td>\n",
              "      <td>-0.399768</td>\n",
              "      <td>4.892519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.268413</td>\n",
              "      <td>-0.399768</td>\n",
              "      <td>-0.295690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.504068</td>\n",
              "      <td>-0.399768</td>\n",
              "      <td>-0.295690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1137</th>\n",
              "      <td>0.882172</td>\n",
              "      <td>-0.399768</td>\n",
              "      <td>-0.295690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1138</th>\n",
              "      <td>-0.504068</td>\n",
              "      <td>-0.399768</td>\n",
              "      <td>4.892519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>-0.504068</td>\n",
              "      <td>-0.399768</td>\n",
              "      <td>0.223131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>-0.504068</td>\n",
              "      <td>4.170729</td>\n",
              "      <td>0.223131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141</th>\n",
              "      <td>-0.504068</td>\n",
              "      <td>-0.399768</td>\n",
              "      <td>-0.295690</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1142 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2\n",
              "0    -0.504068  3.256630 -0.295690\n",
              "1    -0.504068 -0.399768  0.741952\n",
              "2     3.654653 -0.399768  4.892519\n",
              "3     2.268413 -0.399768 -0.295690\n",
              "4    -0.504068 -0.399768 -0.295690\n",
              "...        ...       ...       ...\n",
              "1137  0.882172 -0.399768 -0.295690\n",
              "1138 -0.504068 -0.399768  4.892519\n",
              "1139 -0.504068 -0.399768  0.223131\n",
              "1140 -0.504068  4.170729  0.223131\n",
              "1141 -0.504068 -0.399768 -0.295690\n",
              "\n",
              "[1142 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP-4oVMDvVYd"
      },
      "source": [
        "## 4.3. Building dicts for Train, Train target = 0, Train target = 1 and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iPYibYnhsXz"
      },
      "source": [
        "# Building a dictionnary of contained words and the number of appearence\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "punctuations = string.punctuation\n",
        "\n",
        "def dictio(df, clean=False):\n",
        "  dict = {}\n",
        "  for tweet in df:\n",
        "    if clean:\n",
        "      tweet = cleaning(tweet)\n",
        "    text = simple_preprocess(tweet)\n",
        "    for word in text:\n",
        "      if word not in stop_words:\n",
        "        if word not in dict:\n",
        "          dict[word] = 1\n",
        "        else:\n",
        "          dict[word] +=1\n",
        "  return dict"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Xm8OVLhtDf"
      },
      "source": [
        "# Building dictionnaries\n",
        "\n",
        "# of normal tweets \n",
        "dict_0 = dictio(Train_0['text'])\n",
        "dict_1 = dictio(Train_1['text'])\n",
        "\n",
        "# of cleaned tweets\n",
        "dict_0_cl = dictio(Train_0['text'], True)\n",
        "dict_1_cl = dictio(Train_1['text'], True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsZsPlSn9ztw"
      },
      "source": [
        "# Convert a dic in df\r\n",
        "from collections import OrderedDict\r\n",
        "\r\n",
        "def dic_framing(dict, qty, reverse=False):\r\n",
        "\r\n",
        "  dic = OrderedDict(sorted(dict.items(), key=lambda t: t[1], reverse=reverse))\r\n",
        "\r\n",
        "  top_w = pd.DataFrame()\r\n",
        "\r\n",
        "  words = []\r\n",
        "  numbers = []\r\n",
        "  x=0\r\n",
        "  for word in dic:\r\n",
        "    words.append(word)\r\n",
        "    numbers.append(dic[word])\r\n",
        "    x+=1\r\n",
        "    if x==qty:\r\n",
        "      break\r\n",
        "  \r\n",
        "  top_w['word'] = words\r\n",
        "  top_w['number'] = numbers\r\n",
        "\r\n",
        "  return top_w\r\n",
        "\r\n",
        "# top 10 words of original tweets\r\n",
        "top_0 = dic_framing(dict_0, 30, True)\r\n",
        "top_1 = dic_framing(dict_1, 30, True)\r\n",
        "\r\n",
        "# top 10 words of clean tweets\r\n",
        "top_0_cl = dic_framing(dict_0_cl, 30, True)\r\n",
        "top_1_cl = dic_framing(dict_1_cl, 30, True)\r\n",
        "\r\n",
        "# Dropping the 2 first top words (website components)\r\n",
        "top_0.drop(top_0.index[:2], inplace=True)\r\n",
        "top_1.drop(top_1.index[:2], inplace=True)\r\n",
        "top_0_cl.drop(top_0_cl.index[:2], inplace=True)\r\n",
        "top_1_cl.drop(top_1_cl.index[:2], inplace=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUWEhnpC9u_P"
      },
      "source": [
        "## 4.4. EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iX7eHdrjBkMa",
        "outputId": "e55efd38-e4f7-4628-dbcc-b46667359342"
      },
      "source": [
        "# plotting Top words per class\r\n",
        "fig, ax = plt.subplots(2,2, figsize=(30,20))\r\n",
        "ax[0,0].bar(top_0['word'], top_0['number'])\r\n",
        "ax[0,1].bar(top_1['word'], top_1['number'])\r\n",
        "ax[1,0].bar(top_0_cl['word'], top_0_cl['number'])\r\n",
        "ax[1,1].bar(top_1_cl['word'], top_1_cl['number'])\r\n",
        "\r\n",
        "ax[0,0].tick_params('x', labelrotation=70)\r\n",
        "ax[0,1].tick_params('x', labelrotation=70)\r\n",
        "ax[1,0].tick_params('x', labelrotation=70)\r\n",
        "ax[1,1].tick_params('x', labelrotation=70)\r\n",
        "\r\n",
        "ax[0,0].set_ylabel('Number of appearence')\r\n",
        "ax[1,0].set_ylabel('Number of appearence')\r\n",
        "\r\n",
        "ax[0,0].set_title('Word appearence in original text : Target = 0')\r\n",
        "ax[0,1].set_title('Word appearence in original text : Target = 1')\r\n",
        "ax[1,0].set_title('Word appearence in cleaned text : Target = 0')\r\n",
        "ax[1,1].set_title('Word appearence in cleaned text : Target = 1')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Word appearence in cleaned text : Target = 1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x1440 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7FqB4eWjQIh"
      },
      "source": [
        "# 5. Tokenizer\n",
        "We tried to build our own tokenizer. But we observed that the gensim.simple_preprocess performed better in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "xTNiQmeSCJPZ",
        "outputId": "4de528b4-2a08-48e7-d21c-3cea1cf4957f"
      },
      "source": [
        "# Cleaned features before tokenization :\n",
        "spl = X.sample(20, random_state=sss)\n",
        "display(spl.values)\n",
        "\n",
        "# Tokenized clean features :\n",
        "for x in spl:\n",
        "  print(simple_preprocess(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([\"eyewitness user hamburg police chief gregory wickett has told  eyewitness news he 'can't confirm or deny' an investigation is underway.\",\n",
              "       'wild fires user watching news of wild fires and hope all is ok.',\n",
              "       'arson arson suspect linked to  fires caught in northern california  website',\n",
              "       'blizzard user user just order a blizzard pay then put your nuts in it say they have you ball flavored. boom free ice cream',\n",
              "       \"screaming my favs i'm screaming so fucking loud  website\",\n",
              "       ' just got sent this photo from ruby #alaska as smoke from #wildfires pours into a school ',\n",
              "       \"injured user you're not injured anymore? ??\",\n",
              "       \"quarantined reddit's new content policy goes into effect many horrible subreddits banned or quarantined  website  website\",\n",
              "       \"bioterrorism user satan's daughter shadow warrior in ft women aka transgender mode ps nyc is about to fold extra extra center of bioterrorism\",\n",
              "       'injured  dead dozens injured in gaza blast near house leveled in summer war    website via  website',\n",
              "       \"deluge user my feed seems to have a deluge once or twice during the week. it's fantastic.\",\n",
              "       'obliterated obliterated my phone screen today with a drum stick. #blessed',\n",
              "       'rioting i think twitter was invented to keep us insomniacs from rioting in the wee small hours.',\n",
              "       'bridge collapse us wont upgrade its infrastructure?  website it a bad situation and its going to get ugly very quickly #usa #sustainability',\n",
              "       'bleeding eating takis then rubbing my eyes with my hands now my eyes are bleeding tears',\n",
              "       'wounded user officer wounded suspect killed in exchange of gunfire  website ushed',\n",
              "       'fire fire waves and darkness',\n",
              "       'refugees captain abbott must go down with lnp boat #refugees #christianvalues  website',\n",
              "       'bombing japan marks th anniversary of hiroshima atomic bombing  website',\n",
              "       'floods user when it rains in nj it flash floods. otherwise its just a desert of grief and taxes.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['eyewitness', 'user', 'hamburg', 'police', 'chief', 'gregory', 'wickett', 'has', 'told', 'eyewitness', 'news', 'he', 'can', 'confirm', 'or', 'deny', 'an', 'investigation', 'is', 'underway']\n",
            "['wild', 'fires', 'user', 'watching', 'news', 'of', 'wild', 'fires', 'and', 'hope', 'all', 'is', 'ok']\n",
            "['arson', 'arson', 'suspect', 'linked', 'to', 'fires', 'caught', 'in', 'northern', 'california', 'website']\n",
            "['blizzard', 'user', 'user', 'just', 'order', 'blizzard', 'pay', 'then', 'put', 'your', 'nuts', 'in', 'it', 'say', 'they', 'have', 'you', 'ball', 'flavored', 'boom', 'free', 'ice', 'cream']\n",
            "['screaming', 'my', 'favs', 'screaming', 'so', 'fucking', 'loud', 'website']\n",
            "['just', 'got', 'sent', 'this', 'photo', 'from', 'ruby', 'alaska', 'as', 'smoke', 'from', 'wildfires', 'pours', 'into', 'school']\n",
            "['injured', 'user', 'you', 're', 'not', 'injured', 'anymore']\n",
            "['quarantined', 'reddit', 'new', 'content', 'policy', 'goes', 'into', 'effect', 'many', 'horrible', 'subreddits', 'banned', 'or', 'quarantined', 'website', 'website']\n",
            "['bioterrorism', 'user', 'satan', 'daughter', 'shadow', 'warrior', 'in', 'ft', 'women', 'aka', 'transgender', 'mode', 'ps', 'nyc', 'is', 'about', 'to', 'fold', 'extra', 'extra', 'center', 'of', 'bioterrorism']\n",
            "['injured', 'dead', 'dozens', 'injured', 'in', 'gaza', 'blast', 'near', 'house', 'leveled', 'in', 'summer', 'war', 'website', 'via', 'website']\n",
            "['deluge', 'user', 'my', 'feed', 'seems', 'to', 'have', 'deluge', 'once', 'or', 'twice', 'during', 'the', 'week', 'it', 'fantastic']\n",
            "['obliterated', 'obliterated', 'my', 'phone', 'screen', 'today', 'with', 'drum', 'stick', 'blessed']\n",
            "['rioting', 'think', 'twitter', 'was', 'invented', 'to', 'keep', 'us', 'insomniacs', 'from', 'rioting', 'in', 'the', 'wee', 'small', 'hours']\n",
            "['bridge', 'collapse', 'us', 'wont', 'upgrade', 'its', 'infrastructure', 'website', 'it', 'bad', 'situation', 'and', 'its', 'going', 'to', 'get', 'ugly', 'very', 'quickly', 'usa', 'sustainability']\n",
            "['bleeding', 'eating', 'takis', 'then', 'rubbing', 'my', 'eyes', 'with', 'my', 'hands', 'now', 'my', 'eyes', 'are', 'bleeding', 'tears']\n",
            "['wounded', 'user', 'officer', 'wounded', 'suspect', 'killed', 'in', 'exchange', 'of', 'gunfire', 'website', 'ushed']\n",
            "['fire', 'fire', 'waves', 'and', 'darkness']\n",
            "['refugees', 'captain', 'abbott', 'must', 'go', 'down', 'with', 'lnp', 'boat', 'refugees', 'christianvalues', 'website']\n",
            "['bombing', 'japan', 'marks', 'th', 'anniversary', 'of', 'hiroshima', 'atomic', 'bombing', 'website']\n",
            "['floods', 'user', 'when', 'it', 'rains', 'in', 'nj', 'it', 'flash', 'floods', 'otherwise', 'its', 'just', 'desert', 'of', 'grief', 'and', 'taxes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H8bp8SHi8Gn"
      },
      "source": [
        "# 6. Optimisation\n",
        "The goal of this step is to find the best vector and the best classifier : with maximal accuracy on validation set (accuracy of a set which was not used in the building of the model) by spliting X and y in X_train, X_test (validation), y_train, y_test (validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7BBms3pzU3A"
      },
      "source": [
        "## 6.1. (BEST MODEL) LogReg ---------- MAX ACCURACY = 0.8147\n",
        "Vector : TFIDF\n",
        "* n_gram : (1, 1)\n",
        "* min_df : 3\n",
        "* max_df : 1.0\n",
        "* analyzer : 'word'\n",
        "* sublinear_tf : True\n",
        "* norm : 'l2'\n",
        "\n",
        "Classifier parameters :\n",
        "\n",
        "*   C : 1.21\n",
        "\n",
        "VALIDATION ACCURACY = 0.8147\n",
        "\n",
        "REAL ACCURACY = 0.832"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtZE8CB7jv3e"
      },
      "source": [
        "#### 6.1.1. LogReg TFIDF ---------- MAX ACCURACY = 0.8147 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ysMJUWECPWr",
        "outputId": "ba7617c1-1417-4a89-bf11-39e184fd5331"
      },
      "source": [
        "# Create list of configs\n",
        "def configs():\n",
        "\n",
        "    models = list()\n",
        "    \n",
        "    # Define config lists\n",
        "    ngram_range = [(1,1)] #0  [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]\n",
        "    min_df = [3] #1 [0, 1, 2, 3, 4, 5]\n",
        "    max_df = [1.0] #2 [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    analyzer=['word'] #3  ['word', 'char']\n",
        "    sublinear_tf = [True] #4  [True, False]\n",
        "    norm = ['l2'] #5  ['l1', 'l2']\n",
        "    C = [1.3738237958832638] #6 np.logspace(-4, 4, 20)\n",
        "\n",
        "\n",
        "    \n",
        "    # Create config instances\n",
        "    for n in ngram_range:\n",
        "        for i in min_df:\n",
        "            for j in max_df:\n",
        "              for a in analyzer:\n",
        "                for s in sublinear_tf:\n",
        "                  for no in norm:\n",
        "                    for c in C:\n",
        "                      cfg = [n, i, j, a, s, no, c]\n",
        "                      models.append(cfg)\n",
        "    return models\n",
        "\n",
        "configs = configs()\n",
        "print(configs)\n",
        "print(len(configs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(1, 1), 3, 1.0, 'word', True, 'l2', 1.3738237958832638], [(1, 1), 3, 1.0, 'word', False, 'l2', 1.3738237958832638]]\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fYQB9WOciO4"
      },
      "source": [
        "**TO BEAT :**\n",
        "\n",
        "(1, 1), 3, 1.0, 'word', True, 'l2', 1.3738237958832638\n",
        "ACCURACY SCORE = 0.8146718146718147 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74It3LP_CTeu",
        "outputId": "a6ef8847-4d96-4257-bb77-45a86f22025a"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Define list for result\n",
        "result = []\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "  # Redefine vectorizer\n",
        "  tfidf_vector = TfidfVectorizer(tokenizer=simple_preprocess, \n",
        "                                ngram_range=config[0],\n",
        "                                min_df=config[1],\n",
        "                                 max_df=config[2], \n",
        "                                 analyzer=config[3], \n",
        "                                 sublinear_tf=config[4], \n",
        "                                 norm=config[5], \n",
        "                                 encoding='latin-1')\n",
        "\n",
        "  # Define classifier\n",
        "  classifier = LogisticRegression(C=config[6], max_iter=1000, n_jobs=cores, random_state=SEED, solver='lbfgs')\n",
        "\n",
        "  # Create pipeline\n",
        "  pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "  # Fit model on training set\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions\n",
        "  y_pred = pipe.predict(X_test)\n",
        "\n",
        "  # Print accuracy on test set\n",
        "  print(\"CONFIG: \", config)\n",
        "  print(f\"TEST ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "  print(\"-----------------------\")\n",
        "\n",
        "  # Append to result\n",
        "  result.append([config, accuracy_score(y_test, y_pred)])\n",
        "\n",
        "result.sort(key=lambda x:x[1])\n",
        "print('Top parameters :\\n')\n",
        "print(result[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 1), 3, 1.0, 'word', True, 'l2', 1.3738237958832638]\n",
            "TEST ACCURACY SCORE:\n",
            "0.8147\n",
            "TEST F1 SCORE:\n",
            "0.7683\n",
            "TEST CONFUSION MATRIX:\n",
            "[[657  90]\n",
            " [150 398]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 1), 3, 1.0, 'word', False, 'l2', 1.3738237958832638]\n",
            "TEST ACCURACY SCORE:\n",
            "0.8093\n",
            "TEST F1 SCORE:\n",
            "0.7618\n",
            "TEST CONFUSION MATRIX:\n",
            "[[653  94]\n",
            " [153 395]]\n",
            "-----------------------\n",
            "Top parameters :\n",
            "\n",
            "[[(1, 1), 3, 1.0, 'word', True, 'l2', 1.3738237958832638], 0.8146718146718147]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpM2Sijsj1Ap"
      },
      "source": [
        "#### 6.1.2. LogReg CountVector ---------- MAX ACCURACY = 0.8008"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEKj8u6Qj5Qz",
        "outputId": "57494c69-bcdb-4524-e98c-3b4558c4a086"
      },
      "source": [
        "# Create list of configs\n",
        "def configs():\n",
        "\n",
        "    models = list()\n",
        "    \n",
        "    # Define config lists\n",
        "    ngram_range = [(1,3)] #0  [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]\n",
        "    min_df = [2] #1 [0, 1, 2, 3, 4, 5]\n",
        "    max_df = [0.5] #2 [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    analyzer=['word'] #3  ['word', 'char', 'char_wb']\n",
        "    C = [0.08858667904100823] #4 np.logspace(-4, 4, 20)\n",
        "\n",
        "\n",
        "    \n",
        "    # Create config instances\n",
        "    for a in ngram_range:\n",
        "      for b in min_df:\n",
        "        for c in max_df:\n",
        "          for d in analyzer:\n",
        "            for e in C:\n",
        "              cfg = [a, b, c, d, e]\n",
        "              models.append(cfg)\n",
        "    return models\n",
        "\n",
        "configs = configs()\n",
        "print(configs)\n",
        "print(len(configs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(1, 1), 1, 1.0, 'word', 1.4]]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nLuB11EqK_Z"
      },
      "source": [
        "**TO BEAT :**\n",
        "\n",
        "(1, 3), 2, 0.5, 'word', 0.08858667904100823\n",
        "ACCURACY SCORE = 0.8007722007722008"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl1z4kYQj8I7",
        "outputId": "503bd8b8-4cf7-4b8b-de3f-b645a56b3f37"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Define list for result\n",
        "result = []\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "  # Redefine vectorizer\n",
        "  tfidf_vector = CountVectorizer(tokenizer=simple_preprocess, \n",
        "                                ngram_range=config[0],\n",
        "                                min_df=config[1],\n",
        "                                max_df=config[2], \n",
        "                                analyzer=config[3], \n",
        "                                encoding='latin-1')\n",
        "\n",
        "  # Define classifier\n",
        "  classifier = LogisticRegression(C=config[4], max_iter=1000, n_jobs=cores, random_state=SEED, solver='lbfgs')\n",
        "\n",
        "  # Create pipeline\n",
        "  pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "  # Fit model on training set\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions\n",
        "  y_pred = pipe.predict(X_test)\n",
        "\n",
        "  # Print accuracy on test set\n",
        "  print(\"CONFIG: \", config)\n",
        "  print(f\"TEST ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "  print(\"-----------------------\")\n",
        "\n",
        "  # Append to result\n",
        "  result.append([config, accuracy_score(y_test, y_pred)])\n",
        "\n",
        "result.sort(key=lambda x:x[1])\n",
        "print('Top parameters :\\n')\n",
        "print(result[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 1), 1, 1.0, 'word', 1.4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7792\n",
            "TEST F1 SCORE:\n",
            "0.7342\n",
            "TEST CONFUSION MATRIX:\n",
            "[[614 133]\n",
            " [153 395]]\n",
            "-----------------------\n",
            "Top parameters :\n",
            "\n",
            "[[(1, 1), 1, 1.0, 'word', 1.4], 0.7791505791505792]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viclgjIrzYSN"
      },
      "source": [
        "## 6.2. Decision Tree ---------- MAX ACCURACY = 0.7598\n",
        "\n",
        "Vector : TFIDF\n",
        "\n",
        "\n",
        "* n_gram : (1, 2)\n",
        "* min_df : 0\n",
        "* max_df : 1.0\n",
        "* analyzer : 'word'\n",
        "* sublinear_tf : False\n",
        "* norm : 'l2'\n",
        "\n",
        "Classifier parameters :\n",
        "\n",
        "*   criterion : 'entropy'\n",
        "*   splitter : 'random'\n",
        "*   max_depth : None\n",
        "*   min_impurity_decrease : 0.0\n",
        "*   min_impurity_split : 0.2\n",
        "*   min_samples_leaf : 2\n",
        "*   max_leaf_nodes : None\n",
        "*   max_features : 'auto'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2_bgb25yk62"
      },
      "source": [
        "### 6.2.1. Decision Tree TFIDF ---------- MAX ACCURACY = 0.7598"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLJdw83y2Ws1",
        "outputId": "5a520bc3-c960-44ef-d20d-582ee63ec27a"
      },
      "source": [
        "# Create list of configs\n",
        "def configs():\n",
        "\n",
        "    models = list()\n",
        "    \n",
        "    # Define config lists\n",
        "    ngram_range = [(1,2)] #0  [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]\n",
        "    \n",
        "    min_df = [0] #1 [0, 1, 2, 3, 4, 5]\n",
        "    max_df = [1.0] #2 [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    \n",
        "    analyzer=['word'] #3  ['word', 'char']\n",
        "    sublinear_tf = [True, False] #4  [True, False]\n",
        "    norm = ['l2'] #5  ['l1', 'l2']\n",
        "\n",
        "    crit = ['entropy'] #6 ['gini', 'entropy']\n",
        "    split = ['random'] #7 ['best', 'random']\n",
        "    \n",
        "    maxdep = [None] #8 [10, 50, 100, 200, 500, 1000, None]\n",
        "\n",
        "    minimpd = [0.0] #9 [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "    minimps = [0.2] #10 [None, 0.05, 0.1, 0.15, 0.2]\n",
        "    minspl = [2] #11 [1, 2, 3, 4, 5]\n",
        "    maxleaf = [None] #12 [None, 2, 3, 4, 5]\n",
        "    \n",
        "    maxfeat = ['auto'] #13 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 'auto']\n",
        "\n",
        "\n",
        "    \n",
        "    # Create config instances\n",
        "    for a in ngram_range:\n",
        "      for b in min_df:\n",
        "        for c in max_df:\n",
        "          for d in analyzer:\n",
        "            for e in sublinear_tf:\n",
        "              for f in norm:\n",
        "                for g in crit:\n",
        "                  for h in split:\n",
        "                    for i in maxdep:\n",
        "                      for j in minimpd:\n",
        "                        for k in minimps:\n",
        "                          for l in minspl:\n",
        "                            for m in maxleaf:\n",
        "                              for n in maxfeat:\n",
        "                                cfg = [a, b, c, d, e, f, g, h, i, j, k, l, m, n]\n",
        "                                models.append(cfg)\n",
        "    return models\n",
        "\n",
        "configs = configs()\n",
        "print(configs)\n",
        "print(len(configs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 1], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 2], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 3], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 4], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 5], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 6], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 7], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 8], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 9], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 10], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 15], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 20], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 25], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 30], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 35], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 40], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 45], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 50], [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 'auto'], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 1], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 2], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 3], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 4], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 5], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 6], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 7], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 8], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 9], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 10], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 15], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 20], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 25], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 30], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 35], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 40], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 45], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 50], [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 'auto']]\n",
            "38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxyC4yuE4AIG"
      },
      "source": [
        "**TO BEAT :**\n",
        "\n",
        "[[(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 'auto'], 0.7598455598455598, 0.6797116374871266] \n",
        "\n",
        "[[(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.05, 1, None, 'auto'], 0.7498069498069498, 0.6823529411764706]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD55G3jK2XYs",
        "outputId": "2ec8eac5-6676-4038-a87d-294840cec680"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Define list for result\n",
        "result = []\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "  # Redefine vectorizer\n",
        "  tfidf_vector = TfidfVectorizer(tokenizer=simple_preprocess, \n",
        "                                ngram_range=config[0],\n",
        "                                min_df=config[1],\n",
        "                                 max_df=config[2], \n",
        "                                 analyzer=config[3], \n",
        "                                 sublinear_tf=config[4], \n",
        "                                 norm=config[5], \n",
        "                                 encoding='latin-1')\n",
        "\n",
        "  # Define classifier\n",
        "  classifier = tree.DecisionTreeClassifier(criterion=config[6], splitter=config[7], max_depth=config[8], min_impurity_decrease=config[9],\n",
        "                                           min_impurity_split=config[10], min_samples_leaf=config[11], max_leaf_nodes=config[12], max_features=config[13],\n",
        "                                           random_state=SEED)\n",
        "\n",
        "  # Create pipeline\n",
        "  pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "  # Fit model on training set\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions\n",
        "  y_pred = pipe.predict(X_test)\n",
        "\n",
        "  # Print accuracy on test set\n",
        "  print(\"CONFIG: \", config)\n",
        "  print(f\"TEST ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "  print(\"-----------------------\")\n",
        "\n",
        "  # Append to result\n",
        "  result.append([config, accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)])\n",
        "\n",
        "result.sort(key=lambda x:x[1])\n",
        "print('Top parameters (accuracy) :\\n')\n",
        "print(result[-1], '\\n')\n",
        "result.sort(key=lambda x:x[2])\n",
        "print('Top parameters (F1) :\\n')\n",
        "print(result[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5807\n",
            "TEST F1 SCORE:\n",
            "0.0286\n",
            "TEST CONFUSION MATRIX:\n",
            "[[744   3]\n",
            " [540   8]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5869\n",
            "TEST F1 SCORE:\n",
            "0.0497\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [534  14]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6270\n",
            "TEST F1 SCORE:\n",
            "0.3030\n",
            "TEST CONFUSION MATRIX:\n",
            "[[707  40]\n",
            " [443 105]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6849\n",
            "TEST F1 SCORE:\n",
            "0.4688\n",
            "TEST CONFUSION MATRIX:\n",
            "[[707  40]\n",
            " [368 180]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6710\n",
            "TEST F1 SCORE:\n",
            "0.4148\n",
            "TEST CONFUSION MATRIX:\n",
            "[[718  29]\n",
            " [397 151]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6687\n",
            "TEST F1 SCORE:\n",
            "0.4348\n",
            "TEST CONFUSION MATRIX:\n",
            "[[701  46]\n",
            " [383 165]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6865\n",
            "TEST F1 SCORE:\n",
            "0.4821\n",
            "TEST CONFUSION MATRIX:\n",
            "[[700  47]\n",
            " [359 189]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6618\n",
            "TEST F1 SCORE:\n",
            "0.5989\n",
            "TEST CONFUSION MATRIX:\n",
            "[[530 217]\n",
            " [221 327]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', True, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.7351\n",
            "TEST F1 SCORE:\n",
            "0.6251\n",
            "TEST CONFUSION MATRIX:\n",
            "[[666  81]\n",
            " [262 286]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5807\n",
            "TEST F1 SCORE:\n",
            "0.0286\n",
            "TEST CONFUSION MATRIX:\n",
            "[[744   3]\n",
            " [540   8]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5876\n",
            "TEST F1 SCORE:\n",
            "0.0498\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [534  14]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6263\n",
            "TEST F1 SCORE:\n",
            "0.2644\n",
            "TEST CONFUSION MATRIX:\n",
            "[[724  23]\n",
            " [461  87]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6232\n",
            "TEST F1 SCORE:\n",
            "0.2204\n",
            "TEST CONFUSION MATRIX:\n",
            "[[738   9]\n",
            " [479  69]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6517\n",
            "TEST F1 SCORE:\n",
            "0.3279\n",
            "TEST CONFUSION MATRIX:\n",
            "[[734  13]\n",
            " [438 110]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6579\n",
            "TEST F1 SCORE:\n",
            "0.4313\n",
            "TEST CONFUSION MATRIX:\n",
            "[[684  63]\n",
            " [380 168]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6842\n",
            "TEST F1 SCORE:\n",
            "0.4816\n",
            "TEST CONFUSION MATRIX:\n",
            "[[696  51]\n",
            " [358 190]]\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6911\n",
            "TEST F1 SCORE:\n",
            "0.4975\n",
            "TEST CONFUSION MATRIX:\n",
            "[[697  50]\n",
            " [350 198]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.7598\n",
            "TEST F1 SCORE:\n",
            "0.6797\n",
            "TEST CONFUSION MATRIX:\n",
            "[[654  93]\n",
            " [218 330]]\n",
            "-----------------------\n",
            "Top parameters (accuracy) :\n",
            "\n",
            "[[(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 'auto'], 0.7598455598455598, 0.6797116374871266] \n",
            "\n",
            "Top parameters (F1) :\n",
            "\n",
            "[[(1, 2), 0, 1.0, 'word', False, 'l2', 'entropy', 'random', None, 0.0, 0.2, 2, None, 'auto'], 0.7598455598455598, 0.6797116374871266]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py:301: FutureWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4mx2fsUrPPY"
      },
      "source": [
        "### 6.2.2. Decision Tree CountVector ---------- MAX ACCURACY = 0.7544\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj-LtAr0rV_1",
        "outputId": "d4697276-30d8-450d-ea41-3058d4e24649"
      },
      "source": [
        "# Create list of configs\n",
        "def configs():\n",
        "\n",
        "    models = list()\n",
        "    \n",
        "    # Define config lists\n",
        "    ngram_range = [(1,3)] #0  [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]\n",
        "    min_df = [1] #1 [0, 1, 2, 3, 4, 5]\n",
        "    max_df = [0.5] #2 [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    analyzer=['word'] #3  ['word', 'char', 'char_wb']\n",
        "\n",
        "    crit = ['entropy'] #4 ['gini', 'entropy']\n",
        "    split = ['random'] #5 ['best', 'random']\n",
        "    maxdep = [None] #6 [10, 50, 100, 200, 500, 1000, None]\n",
        "    minimpd = [0.0] #7 [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "    minimps = [None] #8 [None, 0.05, 0.1, 0.15, 0.2]\n",
        "    minspl = [1] #9 [1, 2, 3, 4, 5]\n",
        "    maxleaf = [None] #10 [None, 2, 3, 4, 5]\n",
        "    maxfeat = ['auto'] #11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 'auto']\n",
        "\n",
        "    # Create config instances\n",
        "    for a in ngram_range:\n",
        "      for b in min_df:\n",
        "        for c in max_df:\n",
        "          for d in analyzer:\n",
        "            for e in crit:\n",
        "              for f in split:\n",
        "                for g in maxdep:\n",
        "                  for h in minimpd:\n",
        "                    for i in minimps:\n",
        "                      for j in minspl:\n",
        "                        for k in maxleaf:\n",
        "                          for l in maxfeat:\n",
        "                            cfg = [a, b, c, d, e, f, g, h, i, j, k, l]\n",
        "                            models.append(cfg)\n",
        "    return models\n",
        "\n",
        "configs = configs()\n",
        "print(configs, '\\n')\n",
        "print('Number of configs :', len(configs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 1], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 2], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 3], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 4], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 5], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 6], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 7], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 8], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 9], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 10], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 15], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 20], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 25], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 30], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 35], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 40], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 45], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 50], [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 'auto']] \n",
            "\n",
            "Number of configs : 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLbo_UeJvm_O"
      },
      "source": [
        "**TO BEAT :**\n",
        "\n",
        "[[(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 'auto'], 0.7544401544401544]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpKFPzmRrV5R",
        "outputId": "fd531bb8-5c5d-4c03-b367-01793809e632"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Define list for result\n",
        "result = []\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "  # Redefine vectorizer\n",
        "  tfidf_vector = CountVectorizer(tokenizer=simple_preprocess, \n",
        "                                ngram_range=config[0],\n",
        "                                min_df=config[1],\n",
        "                                max_df=config[2], \n",
        "                                analyzer=config[3], \n",
        "                                encoding='latin-1')\n",
        "\n",
        "  # Define classifier\n",
        "  classifier = tree.DecisionTreeClassifier(criterion=config[4], splitter=config[5], max_depth=config[6], min_impurity_decrease=config[7],\n",
        "                                           min_impurity_split=config[8], min_samples_leaf=config[9], max_leaf_nodes=config[10], max_features=config[11],\n",
        "                                           random_state=SEED)\n",
        "\n",
        "  # Create pipeline\n",
        "  pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "  # Fit model on training set\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions\n",
        "  y_pred = pipe.predict(X_test)\n",
        "\n",
        "  # Print accuracy on test set\n",
        "  print(\"CONFIG: \", config)\n",
        "  print(f\"TEST ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "  print(\"-----------------------\")\n",
        "\n",
        "  # Append to result\n",
        "  result.append([config, accuracy_score(y_test, y_pred)])\n",
        "\n",
        "result.sort(key=lambda x:x[1])\n",
        "print('Top parameters :\\n')\n",
        "print(result[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6764\n",
            "TEST F1 SCORE:\n",
            "0.5814\n",
            "TEST CONFUSION MATRIX:\n",
            "[[585 162]\n",
            " [257 291]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6695\n",
            "TEST F1 SCORE:\n",
            "0.6088\n",
            "TEST CONFUSION MATRIX:\n",
            "[[534 213]\n",
            " [215 333]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6880\n",
            "TEST F1 SCORE:\n",
            "0.5800\n",
            "TEST CONFUSION MATRIX:\n",
            "[[612 135]\n",
            " [269 279]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7019\n",
            "TEST F1 SCORE:\n",
            "0.6132\n",
            "TEST CONFUSION MATRIX:\n",
            "[[603 144]\n",
            " [242 306]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7042\n",
            "TEST F1 SCORE:\n",
            "0.5711\n",
            "TEST CONFUSION MATRIX:\n",
            "[[657  90]\n",
            " [293 255]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6525\n",
            "TEST F1 SCORE:\n",
            "0.5996\n",
            "TEST CONFUSION MATRIX:\n",
            "[[508 239]\n",
            " [211 337]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6919\n",
            "TEST F1 SCORE:\n",
            "0.5562\n",
            "TEST CONFUSION MATRIX:\n",
            "[[646 101]\n",
            " [298 250]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6903\n",
            "TEST F1 SCORE:\n",
            "0.5757\n",
            "TEST CONFUSION MATRIX:\n",
            "[[622 125]\n",
            " [276 272]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6965\n",
            "TEST F1 SCORE:\n",
            "0.6050\n",
            "TEST CONFUSION MATRIX:\n",
            "[[601 146]\n",
            " [247 301]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7035\n",
            "TEST F1 SCORE:\n",
            "0.5862\n",
            "TEST CONFUSION MATRIX:\n",
            "[[639 108]\n",
            " [276 272]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6834\n",
            "TEST F1 SCORE:\n",
            "0.5764\n",
            "TEST CONFUSION MATRIX:\n",
            "[[606 141]\n",
            " [269 279]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6819\n",
            "TEST F1 SCORE:\n",
            "0.5492\n",
            "TEST CONFUSION MATRIX:\n",
            "[[632 115]\n",
            " [297 251]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6981\n",
            "TEST F1 SCORE:\n",
            "0.5755\n",
            "TEST CONFUSION MATRIX:\n",
            "[[639 108]\n",
            " [283 265]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7120\n",
            "TEST F1 SCORE:\n",
            "0.5842\n",
            "TEST CONFUSION MATRIX:\n",
            "[[660  87]\n",
            " [286 262]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7120\n",
            "TEST F1 SCORE:\n",
            "0.5860\n",
            "TEST CONFUSION MATRIX:\n",
            "[[658  89]\n",
            " [284 264]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7166\n",
            "TEST F1 SCORE:\n",
            "0.6015\n",
            "TEST CONFUSION MATRIX:\n",
            "[[651  96]\n",
            " [271 277]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7197\n",
            "TEST F1 SCORE:\n",
            "0.5998\n",
            "TEST CONFUSION MATRIX:\n",
            "[[660  87]\n",
            " [276 272]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7290\n",
            "TEST F1 SCORE:\n",
            "0.6156\n",
            "TEST CONFUSION MATRIX:\n",
            "[[663  84]\n",
            " [267 281]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.7544\n",
            "TEST F1 SCORE:\n",
            "0.6722\n",
            "TEST CONFUSION MATRIX:\n",
            "[[651  96]\n",
            " [222 326]]\n",
            "-----------------------\n",
            "Top parameters :\n",
            "\n",
            "[[(1, 3), 1, 0.5, 'word', 'entropy', 'random', None, 0.0, None, 1, None, 'auto'], 0.7544401544401544]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bvhNUPlMmlo"
      },
      "source": [
        "## 6.3. Random tree ---------- MAX ACCURACY = 0.7954\n",
        "\n",
        "Vector : CountVector\n",
        "\n",
        "* n_gram : (1, 3)\n",
        "* min_df : 3\n",
        "* max_df : 0.5\n",
        "* analyzer : 'char'\n",
        "\n",
        "Classifier parameters :\n",
        "\n",
        "*   criterion : 'gini'\n",
        "*   max_depth : None\n",
        "*   min_impurity_decrease : 0\n",
        "*   min_impurity_split : None\n",
        "*   min_samples_leaf : 1\n",
        "*   max_leaf_nodes : None\n",
        "*   max_features : 'auto'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPacBQY86QaG"
      },
      "source": [
        "### 6.3.1. Random tree TFIDF ---------- MAX ACCURACY = 0.7900"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gXvm1lw6Uhb",
        "outputId": "0ee08bef-1339-4fd4-f4b0-f10cbd9a055d"
      },
      "source": [
        "# Create list of configs\n",
        "def configs():\n",
        "\n",
        "    models = list()\n",
        "    \n",
        "    # Define config lists\n",
        "    ngram_range = [(1,1)] #0  [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]\n",
        "    \n",
        "    min_df = [4] #1 [0, 1, 2, 3, 4, 5]\n",
        "    max_df = [0.5] #2 [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "\n",
        "    analyzer=['word'] #3  ['word', 'char']\n",
        "    sublinear_tf = [True] #4  [True, False]\n",
        "    norm = ['l2'] #5  ['l1', 'l2']\n",
        "\n",
        "    nest = [40] #14 [1,5,10,20,40,80,160,320,640,1280]\n",
        "\n",
        "    crit = ['gini'] #6 ['gini', 'entropy']\n",
        "    \n",
        "    maxdep = [None] #8 [10, 50, 100, 200, 500, 1000, None]\n",
        "\n",
        "    minimpd = [0.0] #9 [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "    minimps = [None] #10 [None, 0.05, 0.1, 0.15, 0.2]\n",
        "    minspl = [1] #11 [1, 2, 3, 4, 5]\n",
        "    maxleaf = [None] #12 [None, 2, 3, 4, 5]\n",
        "    \n",
        "    maxfeat = ['auto'] #13 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 'auto']\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    # Create config instances\n",
        "    for a in ngram_range:\n",
        "      for b in min_df:\n",
        "        for c in max_df:\n",
        "          for d in analyzer:\n",
        "            for e in sublinear_tf:\n",
        "              for f in norm:\n",
        "                for g in crit:\n",
        "                  for i in maxdep:\n",
        "                    for j in minimpd:\n",
        "                      for k in minimps:\n",
        "                        for l in minspl:\n",
        "                          for m in maxleaf:\n",
        "                            for n in maxfeat:\n",
        "                              for o in nest:\n",
        "                                cfg = [a, b, c, d, e, f, g, i, j, k, l, m, n, o]\n",
        "                                models.append(cfg)\n",
        "    return models\n",
        "\n",
        "configs = configs()\n",
        "print(configs)\n",
        "print(len(configs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(1, 1), 4, 0.5, 'word', True, 'l2', 'gini', None, 0.0, None, 1, None, 'auto', 39], [(1, 1), 4, 0.5, 'word', True, 'l2', 'gini', None, 0.0, None, 1, None, 'auto', 40], [(1, 1), 4, 0.5, 'word', True, 'l2', 'gini', None, 0.0, None, 1, None, 'auto', 41]]\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKvLwZFc6reb"
      },
      "source": [
        "**TO BEAT :**\n",
        "[[(1, 1), 4, 0.5, 'word', True, 'l2', 'gini', None, 0.0, None, 1, None, 'auto', 40], 0.78996138996139, 0.7130801687763713]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot8nCLB06rPy",
        "outputId": "d37f87c1-d571-404f-a538-539db6f8d83a"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Define list for result\n",
        "result = []\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "  # Redefine vectorizer\n",
        "  tfidf_vector = TfidfVectorizer(tokenizer=simple_preprocess, \n",
        "                                ngram_range=config[0],\n",
        "                                min_df=config[1],\n",
        "                                 max_df=config[2], \n",
        "                                 analyzer=config[3], \n",
        "                                 sublinear_tf=config[4], \n",
        "                                 norm=config[5], \n",
        "                                 encoding='latin-1')\n",
        "\n",
        "  # Define classifier\n",
        "  classifier = RandomForestClassifier(criterion=config[6], max_depth=config[7], min_impurity_decrease=config[8],\n",
        "                                           min_impurity_split=config[9], min_samples_leaf=config[10], max_leaf_nodes=config[11], max_features=config[12],\n",
        "                                           random_state=SEED, n_estimators=config[13])\n",
        "\n",
        "  # Create pipeline\n",
        "  pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "  # Fit model on training set\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions\n",
        "  y_pred = pipe.predict(X_test)\n",
        "\n",
        "  # Print accuracy on test set\n",
        "  print(\"CONFIG: \", config)\n",
        "  print(f\"TEST ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "  print(\"-----------------------\")\n",
        "\n",
        "  # Append to result\n",
        "  result.append([config, accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)])\n",
        "\n",
        "result.sort(key=lambda x:x[1])\n",
        "print('Top parameters (accuracy) :\\n')\n",
        "print(result[-1], '\\n')\n",
        "result.sort(key=lambda x:x[2])\n",
        "print('Top parameters (F1) :\\n')\n",
        "print(result[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 1), 4, 0.5, 'word', True, 'l2', 'gini', None, 0.0, None, 1, None, 'auto', 39]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7869\n",
            "TEST F1 SCORE:\n",
            "0.7119\n",
            "TEST CONFUSION MATRIX:\n",
            "[[678  69]\n",
            " [207 341]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 1), 4, 0.5, 'word', True, 'l2', 'gini', None, 0.0, None, 1, None, 'auto', 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7900\n",
            "TEST F1 SCORE:\n",
            "0.7131\n",
            "TEST CONFUSION MATRIX:\n",
            "[[685  62]\n",
            " [210 338]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 1), 4, 0.5, 'word', True, 'l2', 'gini', None, 0.0, None, 1, None, 'auto', 41]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7861\n",
            "TEST F1 SCORE:\n",
            "0.7093\n",
            "TEST CONFUSION MATRIX:\n",
            "[[680  67]\n",
            " [210 338]]\n",
            "-----------------------\n",
            "Top parameters (accuracy) :\n",
            "\n",
            "[[(1, 1), 4, 0.5, 'word', True, 'l2', 'gini', None, 0.0, None, 1, None, 'auto', 40], 0.78996138996139, 0.7130801687763713] \n",
            "\n",
            "Top parameters (F1) :\n",
            "\n",
            "[[(1, 1), 4, 0.5, 'word', True, 'l2', 'gini', None, 0.0, None, 1, None, 'auto', 40], 0.78996138996139, 0.7130801687763713]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emaFTvhB6UxY"
      },
      "source": [
        "### 6.3.2. Random tree CountVector ---------- MAX ACCURACY = 0.7954"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYItePDg6Zma",
        "outputId": "6c7e8eb5-0c25-4ac9-8344-30b6680ab1ae"
      },
      "source": [
        "# Create list of configs\n",
        "def configs():\n",
        "\n",
        "    models = list()\n",
        "    \n",
        "    # Define config lists\n",
        "    ngram_range = [(1,3)] #0  [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]\n",
        "    min_df = [3] #1 [0, 1, 2, 3, 4, 5]\n",
        "    max_df = [0.5] #2 [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    analyzer=['char'] #3  ['word', 'char']\n",
        "\n",
        "    nest = [40] #4 [1,5,10,20,40,80,160,320,640,1280]\n",
        "    crit = ['gini'] #5 ['gini', 'entropy']  \n",
        "    maxdep = [None] #6 [10, 50, 100, 200, 500, 1000, None]\n",
        "\n",
        "    minimpd = [0.0] #7 [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "    minimps = [None] #8 [None, 0.05, 0.1, 0.15, 0.2]\n",
        "    minspl = [1] #9 [1, 2, 3, 4, 5]\n",
        "    maxleaf = [None] #10 [None, 2, 3, 4, 5]   \n",
        "    maxfeat = ['auto'] #11 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 'auto']\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    # Create config instances\n",
        "    for a in ngram_range:\n",
        "      for b in min_df:\n",
        "        for c in max_df:\n",
        "          for d in analyzer:\n",
        "            for e in nest:\n",
        "              for f in crit:\n",
        "                for g in maxdep:\n",
        "                  for i in minimpd:\n",
        "                    for j in minimps:\n",
        "                      for k in minspl:\n",
        "                        for l in maxleaf:\n",
        "                          for m in maxfeat:\n",
        "                                cfg = [a, b, c, d, e, f, g, i, j, k, l, m]\n",
        "                                models.append(cfg)\n",
        "    return models\n",
        "\n",
        "configs = configs()\n",
        "print(configs)\n",
        "print(len(configs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 'auto'], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 1], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 2], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 3], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 4], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 5], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 6], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 7], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 8], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 9], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 10], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 15], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 20], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 25], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 30], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 35], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 40], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 45], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 50], [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 'auto']]\n",
            "475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuSwT1Gh6_jH"
      },
      "source": [
        "**TO BEAT :**\n",
        "[[(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 'auto'], 0.7953667953667953]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJzBBbBy6_WG",
        "outputId": "bc190017-d694-4196-9e21-6f2b44b86606"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Define list for result\n",
        "result = []\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "  # Redefine vectorizer\n",
        "  tfidf_vector = CountVectorizer(tokenizer=simple_preprocess, \n",
        "                                ngram_range=config[0],\n",
        "                                min_df=config[1],\n",
        "                                max_df=config[2], \n",
        "                                analyzer=config[3], \n",
        "                                encoding='latin-1')\n",
        "\n",
        "  # Define classifier\n",
        "  classifier = RandomForestClassifier(criterion=config[5], max_depth=config[6], min_impurity_decrease=config[7],\n",
        "                                           min_impurity_split=config[8], min_samples_leaf=config[9], max_leaf_nodes=config[10], max_features=config[11],\n",
        "                                           random_state=SEED, n_estimators=config[4])\n",
        "\n",
        "  # Create pipeline\n",
        "  pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "  # Fit model on training set\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions\n",
        "  y_pred = pipe.predict(X_test)\n",
        "\n",
        "  # Print accuracy on test set\n",
        "  print(\"CONFIG: \", config)\n",
        "  print(f\"TEST ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "  print(\"-----------------------\")\n",
        "\n",
        "  # Append to result\n",
        "  result.append([config, accuracy_score(y_test, y_pred)])\n",
        "\n",
        "result.sort(key=lambda x:x[1])\n",
        "print('Top parameters :\\n')\n",
        "print(result[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7351\n",
            "TEST F1 SCORE:\n",
            "0.6053\n",
            "TEST CONFUSION MATRIX:\n",
            "[[689  58]\n",
            " [285 263]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7529\n",
            "TEST F1 SCORE:\n",
            "0.6244\n",
            "TEST CONFUSION MATRIX:\n",
            "[[709  38]\n",
            " [282 266]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7514\n",
            "TEST F1 SCORE:\n",
            "0.6357\n",
            "TEST CONFUSION MATRIX:\n",
            "[[692  55]\n",
            " [267 281]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7606\n",
            "TEST F1 SCORE:\n",
            "0.6501\n",
            "TEST CONFUSION MATRIX:\n",
            "[[697  50]\n",
            " [260 288]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7521\n",
            "TEST F1 SCORE:\n",
            "0.6453\n",
            "TEST CONFUSION MATRIX:\n",
            "[[682  65]\n",
            " [256 292]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7691\n",
            "TEST F1 SCORE:\n",
            "0.6725\n",
            "TEST CONFUSION MATRIX:\n",
            "[[689  58]\n",
            " [241 307]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7606\n",
            "TEST F1 SCORE:\n",
            "0.6548\n",
            "TEST CONFUSION MATRIX:\n",
            "[[691  56]\n",
            " [254 294]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7544\n",
            "TEST F1 SCORE:\n",
            "0.6513\n",
            "TEST CONFUSION MATRIX:\n",
            "[[680  67]\n",
            " [251 297]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7668\n",
            "TEST F1 SCORE:\n",
            "0.6717\n",
            "TEST CONFUSION MATRIX:\n",
            "[[684  63]\n",
            " [239 309]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7691\n",
            "TEST F1 SCORE:\n",
            "0.6711\n",
            "TEST CONFUSION MATRIX:\n",
            "[[691  56]\n",
            " [243 305]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7761\n",
            "TEST F1 SCORE:\n",
            "0.6861\n",
            "TEST CONFUSION MATRIX:\n",
            "[[688  59]\n",
            " [231 317]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7676\n",
            "TEST F1 SCORE:\n",
            "0.6801\n",
            "TEST CONFUSION MATRIX:\n",
            "[[674  73]\n",
            " [228 320]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7683\n",
            "TEST F1 SCORE:\n",
            "0.6862\n",
            "TEST CONFUSION MATRIX:\n",
            "[[667  80]\n",
            " [220 328]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7637\n",
            "TEST F1 SCORE:\n",
            "0.6786\n",
            "TEST CONFUSION MATRIX:\n",
            "[[666  81]\n",
            " [225 323]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7745\n",
            "TEST F1 SCORE:\n",
            "0.6965\n",
            "TEST CONFUSION MATRIX:\n",
            "[[668  79]\n",
            " [213 335]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7668\n",
            "TEST F1 SCORE:\n",
            "0.6874\n",
            "TEST CONFUSION MATRIX:\n",
            "[[661  86]\n",
            " [216 332]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7784\n",
            "TEST F1 SCORE:\n",
            "0.7020\n",
            "TEST CONFUSION MATRIX:\n",
            "[[670  77]\n",
            " [210 338]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7815\n",
            "TEST F1 SCORE:\n",
            "0.7073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[670  77]\n",
            " [206 342]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.7954\n",
            "TEST F1 SCORE:\n",
            "0.7276\n",
            "TEST CONFUSION MATRIX:\n",
            "[[676  71]\n",
            " [194 354]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 2, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5830\n",
            "TEST F1 SCORE:\n",
            "0.0288\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [540   8]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 3, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5946\n",
            "TEST F1 SCORE:\n",
            "0.0806\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5815\n",
            "TEST F1 SCORE:\n",
            "0.0217\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [542   6]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5900\n",
            "TEST F1 SCORE:\n",
            "0.0602\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [531  17]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5923\n",
            "TEST F1 SCORE:\n",
            "0.0737\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [527  21]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6000\n",
            "TEST F1 SCORE:\n",
            "0.1038\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5946\n",
            "TEST F1 SCORE:\n",
            "0.0806\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1136\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [515  33]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1166\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [514  34]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 4, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6224\n",
            "TEST F1 SCORE:\n",
            "0.1944\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [489  59]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5869\n",
            "TEST F1 SCORE:\n",
            "0.0463\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [535  13]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5923\n",
            "TEST F1 SCORE:\n",
            "0.0704\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [528  20]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5938\n",
            "TEST F1 SCORE:\n",
            "0.0804\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5992\n",
            "TEST F1 SCORE:\n",
            "0.1036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6178\n",
            "TEST F1 SCORE:\n",
            "0.1791\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [494  54]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6124\n",
            "TEST F1 SCORE:\n",
            "0.1549\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [502  46]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6170\n",
            "TEST F1 SCORE:\n",
            "0.1733\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [496  52]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6193\n",
            "TEST F1 SCORE:\n",
            "0.1905\n",
            "TEST CONFUSION MATRIX:\n",
            "[[744   3]\n",
            " [490  58]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, 5, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6363\n",
            "TEST F1 SCORE:\n",
            "0.2559\n",
            "TEST CONFUSION MATRIX:\n",
            "[[743   4]\n",
            " [467  81]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6386\n",
            "TEST F1 SCORE:\n",
            "0.2665\n",
            "TEST CONFUSION MATRIX:\n",
            "[[742   5]\n",
            " [463  85]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7012\n",
            "TEST F1 SCORE:\n",
            "0.4706\n",
            "TEST CONFUSION MATRIX:\n",
            "[[736  11]\n",
            " [376 172]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7328\n",
            "TEST F1 SCORE:\n",
            "0.5564\n",
            "TEST CONFUSION MATRIX:\n",
            "[[732  15]\n",
            " [331 217]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7405\n",
            "TEST F1 SCORE:\n",
            "0.5902\n",
            "TEST CONFUSION MATRIX:\n",
            "[[717  30]\n",
            " [306 242]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7552\n",
            "TEST F1 SCORE:\n",
            "0.6194\n",
            "TEST CONFUSION MATRIX:\n",
            "[[720  27]\n",
            " [290 258]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7552\n",
            "TEST F1 SCORE:\n",
            "0.6310\n",
            "TEST CONFUSION MATRIX:\n",
            "[[707  40]\n",
            " [277 271]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7722\n",
            "TEST F1 SCORE:\n",
            "0.6636\n",
            "TEST CONFUSION MATRIX:\n",
            "[[709  38]\n",
            " [257 291]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7598\n",
            "TEST F1 SCORE:\n",
            "0.6494\n",
            "TEST CONFUSION MATRIX:\n",
            "[[696  51]\n",
            " [260 288]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7583\n",
            "TEST F1 SCORE:\n",
            "0.6455\n",
            "TEST CONFUSION MATRIX:\n",
            "[[697  50]\n",
            " [263 285]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7714\n",
            "TEST F1 SCORE:\n",
            "0.6718\n",
            "TEST CONFUSION MATRIX:\n",
            "[[696  51]\n",
            " [245 303]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7730\n",
            "TEST F1 SCORE:\n",
            "0.6790\n",
            "TEST CONFUSION MATRIX:\n",
            "[[690  57]\n",
            " [237 311]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7846\n",
            "TEST F1 SCORE:\n",
            "0.6997\n",
            "TEST CONFUSION MATRIX:\n",
            "[[691  56]\n",
            " [223 325]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7784\n",
            "TEST F1 SCORE:\n",
            "0.6924\n",
            "TEST CONFUSION MATRIX:\n",
            "[[685  62]\n",
            " [225 323]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7699\n",
            "TEST F1 SCORE:\n",
            "0.6850\n",
            "TEST CONFUSION MATRIX:\n",
            "[[673  74]\n",
            " [224 324]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7745\n",
            "TEST F1 SCORE:\n",
            "0.7039\n",
            "TEST CONFUSION MATRIX:\n",
            "[[656  91]\n",
            " [201 347]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7761\n",
            "TEST F1 SCORE:\n",
            "0.6985\n",
            "TEST CONFUSION MATRIX:\n",
            "[[669  78]\n",
            " [212 336]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7861\n",
            "TEST F1 SCORE:\n",
            "0.7106\n",
            "TEST CONFUSION MATRIX:\n",
            "[[678  69]\n",
            " [208 340]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, None, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.7846\n",
            "TEST F1 SCORE:\n",
            "0.7144\n",
            "TEST CONFUSION MATRIX:\n",
            "[[667  80]\n",
            " [199 349]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 2, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5830\n",
            "TEST F1 SCORE:\n",
            "0.0288\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [540   8]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 3, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5946\n",
            "TEST F1 SCORE:\n",
            "0.0806\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5815\n",
            "TEST F1 SCORE:\n",
            "0.0217\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [542   6]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5900\n",
            "TEST F1 SCORE:\n",
            "0.0602\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [531  17]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5923\n",
            "TEST F1 SCORE:\n",
            "0.0737\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [527  21]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6000\n",
            "TEST F1 SCORE:\n",
            "0.1038\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5946\n",
            "TEST F1 SCORE:\n",
            "0.0806\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1136\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [515  33]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1166\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [514  34]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 4, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6224\n",
            "TEST F1 SCORE:\n",
            "0.1944\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [489  59]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5807\n",
            "TEST F1 SCORE:\n",
            "0.0181\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [543   5]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5869\n",
            "TEST F1 SCORE:\n",
            "0.0463\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [535  13]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5923\n",
            "TEST F1 SCORE:\n",
            "0.0704\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [528  20]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5938\n",
            "TEST F1 SCORE:\n",
            "0.0804\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5992\n",
            "TEST F1 SCORE:\n",
            "0.1036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6178\n",
            "TEST F1 SCORE:\n",
            "0.1791\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [494  54]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6124\n",
            "TEST F1 SCORE:\n",
            "0.1549\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [502  46]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6170\n",
            "TEST F1 SCORE:\n",
            "0.1733\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [496  52]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6193\n",
            "TEST F1 SCORE:\n",
            "0.1905\n",
            "TEST CONFUSION MATRIX:\n",
            "[[744   3]\n",
            " [490  58]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 2, 5, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6363\n",
            "TEST F1 SCORE:\n",
            "0.2559\n",
            "TEST CONFUSION MATRIX:\n",
            "[[743   4]\n",
            " [467  81]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6031\n",
            "TEST F1 SCORE:\n",
            "0.1199\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [513  35]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6587\n",
            "TEST F1 SCORE:\n",
            "0.3343\n",
            "TEST CONFUSION MATRIX:\n",
            "[[742   5]\n",
            " [437 111]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6942\n",
            "TEST F1 SCORE:\n",
            "0.4545\n",
            "TEST CONFUSION MATRIX:\n",
            "[[734  13]\n",
            " [383 165]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7212\n",
            "TEST F1 SCORE:\n",
            "0.5354\n",
            "TEST CONFUSION MATRIX:\n",
            "[[726  21]\n",
            " [340 208]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7328\n",
            "TEST F1 SCORE:\n",
            "0.5653\n",
            "TEST CONFUSION MATRIX:\n",
            "[[724  23]\n",
            " [323 225]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7421\n",
            "TEST F1 SCORE:\n",
            "0.5956\n",
            "TEST CONFUSION MATRIX:\n",
            "[[715  32]\n",
            " [302 246]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7645\n",
            "TEST F1 SCORE:\n",
            "0.6391\n",
            "TEST CONFUSION MATRIX:\n",
            "[[720  27]\n",
            " [278 270]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7660\n",
            "TEST F1 SCORE:\n",
            "0.6497\n",
            "TEST CONFUSION MATRIX:\n",
            "[[711  36]\n",
            " [267 281]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7660\n",
            "TEST F1 SCORE:\n",
            "0.6529\n",
            "TEST CONFUSION MATRIX:\n",
            "[[707  40]\n",
            " [263 285]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7776\n",
            "TEST F1 SCORE:\n",
            "0.6771\n",
            "TEST CONFUSION MATRIX:\n",
            "[[705  42]\n",
            " [246 302]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7792\n",
            "TEST F1 SCORE:\n",
            "0.6857\n",
            "TEST CONFUSION MATRIX:\n",
            "[[697  50]\n",
            " [236 312]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7815\n",
            "TEST F1 SCORE:\n",
            "0.6960\n",
            "TEST CONFUSION MATRIX:\n",
            "[[688  59]\n",
            " [224 324]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7676\n",
            "TEST F1 SCORE:\n",
            "0.6746\n",
            "TEST CONFUSION MATRIX:\n",
            "[[682  65]\n",
            " [236 312]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7761\n",
            "TEST F1 SCORE:\n",
            "0.6928\n",
            "TEST CONFUSION MATRIX:\n",
            "[[678  69]\n",
            " [221 327]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7838\n",
            "TEST F1 SCORE:\n",
            "0.7009\n",
            "TEST CONFUSION MATRIX:\n",
            "[[687  60]\n",
            " [220 328]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7815\n",
            "TEST F1 SCORE:\n",
            "0.6986\n",
            "TEST CONFUSION MATRIX:\n",
            "[[684  63]\n",
            " [220 328]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7838\n",
            "TEST F1 SCORE:\n",
            "0.7071\n",
            "TEST CONFUSION MATRIX:\n",
            "[[677  70]\n",
            " [210 338]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, None, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.7846\n",
            "TEST F1 SCORE:\n",
            "0.7103\n",
            "TEST CONFUSION MATRIX:\n",
            "[[674  73]\n",
            " [206 342]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 2, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5830\n",
            "TEST F1 SCORE:\n",
            "0.0288\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [540   8]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 3, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5946\n",
            "TEST F1 SCORE:\n",
            "0.0806\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5815\n",
            "TEST F1 SCORE:\n",
            "0.0217\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [542   6]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5900\n",
            "TEST F1 SCORE:\n",
            "0.0602\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [531  17]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5923\n",
            "TEST F1 SCORE:\n",
            "0.0737\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [527  21]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6000\n",
            "TEST F1 SCORE:\n",
            "0.1038\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5946\n",
            "TEST F1 SCORE:\n",
            "0.0806\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1136\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [515  33]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1166\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [514  34]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 4, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6224\n",
            "TEST F1 SCORE:\n",
            "0.1944\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [489  59]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5807\n",
            "TEST F1 SCORE:\n",
            "0.0181\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [543   5]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5869\n",
            "TEST F1 SCORE:\n",
            "0.0463\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [535  13]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5923\n",
            "TEST F1 SCORE:\n",
            "0.0704\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [528  20]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5938\n",
            "TEST F1 SCORE:\n",
            "0.0804\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5992\n",
            "TEST F1 SCORE:\n",
            "0.1036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6178\n",
            "TEST F1 SCORE:\n",
            "0.1791\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [494  54]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6124\n",
            "TEST F1 SCORE:\n",
            "0.1549\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [502  46]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6170\n",
            "TEST F1 SCORE:\n",
            "0.1733\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [496  52]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6193\n",
            "TEST F1 SCORE:\n",
            "0.1905\n",
            "TEST CONFUSION MATRIX:\n",
            "[[744   3]\n",
            " [490  58]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 3, 5, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6363\n",
            "TEST F1 SCORE:\n",
            "0.2559\n",
            "TEST CONFUSION MATRIX:\n",
            "[[743   4]\n",
            " [467  81]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5869\n",
            "TEST F1 SCORE:\n",
            "0.0463\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [535  13]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6077\n",
            "TEST F1 SCORE:\n",
            "0.1361\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [508  40]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6741\n",
            "TEST F1 SCORE:\n",
            "0.3848\n",
            "TEST CONFUSION MATRIX:\n",
            "[[741   6]\n",
            " [416 132]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7097\n",
            "TEST F1 SCORE:\n",
            "0.4987\n",
            "TEST CONFUSION MATRIX:\n",
            "[[732  15]\n",
            " [361 187]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7174\n",
            "TEST F1 SCORE:\n",
            "0.5133\n",
            "TEST CONFUSION MATRIX:\n",
            "[[736  11]\n",
            " [355 193]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7259\n",
            "TEST F1 SCORE:\n",
            "0.5501\n",
            "TEST CONFUSION MATRIX:\n",
            "[[723  24]\n",
            " [331 217]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7375\n",
            "TEST F1 SCORE:\n",
            "0.5823\n",
            "TEST CONFUSION MATRIX:\n",
            "[[718  29]\n",
            " [311 237]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7452\n",
            "TEST F1 SCORE:\n",
            "0.6062\n",
            "TEST CONFUSION MATRIX:\n",
            "[[711  36]\n",
            " [294 254]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7521\n",
            "TEST F1 SCORE:\n",
            "0.6210\n",
            "TEST CONFUSION MATRIX:\n",
            "[[711  36]\n",
            " [285 263]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7714\n",
            "TEST F1 SCORE:\n",
            "0.6636\n",
            "TEST CONFUSION MATRIX:\n",
            "[[707  40]\n",
            " [256 292]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7691\n",
            "TEST F1 SCORE:\n",
            "0.6718\n",
            "TEST CONFUSION MATRIX:\n",
            "[[690  57]\n",
            " [242 306]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7884\n",
            "TEST F1 SCORE:\n",
            "0.6969\n",
            "TEST CONFUSION MATRIX:\n",
            "[[706  41]\n",
            " [233 315]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7660\n",
            "TEST F1 SCORE:\n",
            "0.6667\n",
            "TEST CONFUSION MATRIX:\n",
            "[[689  58]\n",
            " [245 303]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7730\n",
            "TEST F1 SCORE:\n",
            "0.6859\n",
            "TEST CONFUSION MATRIX:\n",
            "[[680  67]\n",
            " [227 321]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7807\n",
            "TEST F1 SCORE:\n",
            "0.6966\n",
            "TEST CONFUSION MATRIX:\n",
            "[[685  62]\n",
            " [222 326]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7807\n",
            "TEST F1 SCORE:\n",
            "0.6979\n",
            "TEST CONFUSION MATRIX:\n",
            "[[683  64]\n",
            " [220 328]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7884\n",
            "TEST F1 SCORE:\n",
            "0.7047\n",
            "TEST CONFUSION MATRIX:\n",
            "[[694  53]\n",
            " [221 327]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, None, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.7799\n",
            "TEST F1 SCORE:\n",
            "0.7040\n",
            "TEST CONFUSION MATRIX:\n",
            "[[671  76]\n",
            " [209 339]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 2, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5830\n",
            "TEST F1 SCORE:\n",
            "0.0288\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [540   8]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 3, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5946\n",
            "TEST F1 SCORE:\n",
            "0.0806\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5807\n",
            "TEST F1 SCORE:\n",
            "0.0181\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [543   5]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5900\n",
            "TEST F1 SCORE:\n",
            "0.0602\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [531  17]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5915\n",
            "TEST F1 SCORE:\n",
            "0.0703\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [528  20]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6000\n",
            "TEST F1 SCORE:\n",
            "0.1038\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5954\n",
            "TEST F1 SCORE:\n",
            "0.0839\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [524  24]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1136\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [515  33]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1166\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [514  34]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 4, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6224\n",
            "TEST F1 SCORE:\n",
            "0.1944\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [489  59]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5807\n",
            "TEST F1 SCORE:\n",
            "0.0181\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [543   5]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5853\n",
            "TEST F1 SCORE:\n",
            "0.0394\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [537  11]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5861\n",
            "TEST F1 SCORE:\n",
            "0.0463\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [535  13]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5938\n",
            "TEST F1 SCORE:\n",
            "0.0804\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5992\n",
            "TEST F1 SCORE:\n",
            "0.1036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6178\n",
            "TEST F1 SCORE:\n",
            "0.1791\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [494  54]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6139\n",
            "TEST F1 SCORE:\n",
            "0.1611\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [500  48]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6170\n",
            "TEST F1 SCORE:\n",
            "0.1733\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [496  52]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6193\n",
            "TEST F1 SCORE:\n",
            "0.1905\n",
            "TEST CONFUSION MATRIX:\n",
            "[[744   3]\n",
            " [490  58]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 4, 5, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6363\n",
            "TEST F1 SCORE:\n",
            "0.2559\n",
            "TEST CONFUSION MATRIX:\n",
            "[[743   4]\n",
            " [467  81]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6069\n",
            "TEST F1 SCORE:\n",
            "0.1329\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [509  39]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6502\n",
            "TEST F1 SCORE:\n",
            "0.2998\n",
            "TEST CONFUSION MATRIX:\n",
            "[[745   2]\n",
            " [451  97]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6942\n",
            "TEST F1 SCORE:\n",
            "0.4469\n",
            "TEST CONFUSION MATRIX:\n",
            "[[739   8]\n",
            " [388 160]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7081\n",
            "TEST F1 SCORE:\n",
            "0.4864\n",
            "TEST CONFUSION MATRIX:\n",
            "[[738   9]\n",
            " [369 179]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7212\n",
            "TEST F1 SCORE:\n",
            "0.5354\n",
            "TEST CONFUSION MATRIX:\n",
            "[[726  21]\n",
            " [340 208]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7290\n",
            "TEST F1 SCORE:\n",
            "0.5607\n",
            "TEST CONFUSION MATRIX:\n",
            "[[720  27]\n",
            " [324 224]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7413\n",
            "TEST F1 SCORE:\n",
            "0.5890\n",
            "TEST CONFUSION MATRIX:\n",
            "[[720  27]\n",
            " [308 240]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7467\n",
            "TEST F1 SCORE:\n",
            "0.6105\n",
            "TEST CONFUSION MATRIX:\n",
            "[[710  37]\n",
            " [291 257]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7668\n",
            "TEST F1 SCORE:\n",
            "0.6513\n",
            "TEST CONFUSION MATRIX:\n",
            "[[711  36]\n",
            " [266 282]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7614\n",
            "TEST F1 SCORE:\n",
            "0.6493\n",
            "TEST CONFUSION MATRIX:\n",
            "[[700  47]\n",
            " [262 286]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7683\n",
            "TEST F1 SCORE:\n",
            "0.6644\n",
            "TEST CONFUSION MATRIX:\n",
            "[[698  49]\n",
            " [251 297]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7622\n",
            "TEST F1 SCORE:\n",
            "0.6593\n",
            "TEST CONFUSION MATRIX:\n",
            "[[689  58]\n",
            " [250 298]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7737\n",
            "TEST F1 SCORE:\n",
            "0.6832\n",
            "TEST CONFUSION MATRIX:\n",
            "[[686  61]\n",
            " [232 316]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7768\n",
            "TEST F1 SCORE:\n",
            "0.6896\n",
            "TEST CONFUSION MATRIX:\n",
            "[[685  62]\n",
            " [227 321]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7560\n",
            "TEST F1 SCORE:\n",
            "0.6558\n",
            "TEST CONFUSION MATRIX:\n",
            "[[678  69]\n",
            " [247 301]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7768\n",
            "TEST F1 SCORE:\n",
            "0.6909\n",
            "TEST CONFUSION MATRIX:\n",
            "[[683  64]\n",
            " [225 323]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, None, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.7822\n",
            "TEST F1 SCORE:\n",
            "0.7069\n",
            "TEST CONFUSION MATRIX:\n",
            "[[673  74]\n",
            " [208 340]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 2, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5830\n",
            "TEST F1 SCORE:\n",
            "0.0288\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [540   8]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 3, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.5946\n",
            "TEST F1 SCORE:\n",
            "0.0806\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5776\n",
            "TEST F1 SCORE:\n",
            "0.0036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [547   1]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5807\n",
            "TEST F1 SCORE:\n",
            "0.0181\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [543   5]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5900\n",
            "TEST F1 SCORE:\n",
            "0.0602\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [531  17]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5915\n",
            "TEST F1 SCORE:\n",
            "0.0703\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [528  20]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6000\n",
            "TEST F1 SCORE:\n",
            "0.1038\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5954\n",
            "TEST F1 SCORE:\n",
            "0.0839\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [524  24]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1136\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [515  33]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6023\n",
            "TEST F1 SCORE:\n",
            "0.1166\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [514  34]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 4, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6224\n",
            "TEST F1 SCORE:\n",
            "0.1944\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [489  59]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 2]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 3]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 4]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5792\n",
            "TEST F1 SCORE:\n",
            "0.0109\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [545   3]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 5]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 6]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5830\n",
            "TEST F1 SCORE:\n",
            "0.0288\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [540   8]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 7]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5822\n",
            "TEST F1 SCORE:\n",
            "0.0252\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [541   7]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 8]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5807\n",
            "TEST F1 SCORE:\n",
            "0.0181\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [543   5]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 9]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5768\n",
            "TEST F1 SCORE:\n",
            "0.0000\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [548   0]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 10]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5784\n",
            "TEST F1 SCORE:\n",
            "0.0073\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [546   2]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 15]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5807\n",
            "TEST F1 SCORE:\n",
            "0.0181\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [543   5]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 20]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5861\n",
            "TEST F1 SCORE:\n",
            "0.0463\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [535  13]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 25]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5938\n",
            "TEST F1 SCORE:\n",
            "0.0804\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [525  23]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 30]\n",
            "TEST ACCURACY SCORE:\n",
            "0.5992\n",
            "TEST F1 SCORE:\n",
            "0.1036\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [518  30]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 35]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6178\n",
            "TEST F1 SCORE:\n",
            "0.1791\n",
            "TEST CONFUSION MATRIX:\n",
            "[[746   1]\n",
            " [494  54]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 40]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6139\n",
            "TEST F1 SCORE:\n",
            "0.1611\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [500  48]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6170\n",
            "TEST F1 SCORE:\n",
            "0.1733\n",
            "TEST CONFUSION MATRIX:\n",
            "[[747   0]\n",
            " [496  52]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 50]\n",
            "TEST ACCURACY SCORE:\n",
            "0.6193\n",
            "TEST F1 SCORE:\n",
            "0.1905\n",
            "TEST CONFUSION MATRIX:\n",
            "[[744   3]\n",
            " [490  58]]\n",
            "-----------------------\n",
            "CONFIG:  [(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 5, 5, 'auto']\n",
            "TEST ACCURACY SCORE:\n",
            "0.6363\n",
            "TEST F1 SCORE:\n",
            "0.2559\n",
            "TEST CONFUSION MATRIX:\n",
            "[[743   4]\n",
            " [467  81]]\n",
            "-----------------------\n",
            "Top parameters :\n",
            "\n",
            "[[(1, 3), 3, 0.5, 'char', 40, 'gini', None, 0.0, None, 1, None, 'auto'], 0.7953667953667953]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIVf5px9KHBg"
      },
      "source": [
        "## 6.4. KNN ---------- MAX ACCURACY = 0.7969\r\n",
        "\r\n",
        "Vector : TFIDF\r\n",
        "\r\n",
        "* n_gram : (1, 2)\r\n",
        "* min_df : 2\r\n",
        "* max_df : 1.0\r\n",
        "* analyzer : 'word'\r\n",
        "\r\n",
        "Classifier parameters :\r\n",
        "\r\n",
        "*   n : 45\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gd8RLLH6fM6"
      },
      "source": [
        "### 6.4.1. KNN TFIDF ---------- MAX ACCURACY = 0.7969"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYWQWhiq6h0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901d457b-8e04-4fc0-dedf-3e6b7d076ea0"
      },
      "source": [
        "# Create list of configs\n",
        "def configs():\n",
        "\n",
        "    models = list()\n",
        "    \n",
        "    # Define config lists\n",
        "    ngram_range = [(1,2)] #0  [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]\n",
        "    \n",
        "    min_df = [2] #1 [0, 1, 2, 3, 4, 5]\n",
        "    max_df = [1.0] #2 [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    \n",
        "    analyzer=['word'] #3  ['word', 'char']\n",
        "    sublinear_tf = [True] #4  [True, False]\n",
        "    norm = ['l2'] #5  ['l1', 'l2']\n",
        "\n",
        "    n = [45] #6  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 150, 200]\n",
        "\n",
        "\n",
        "    \n",
        "    # Create config instances\n",
        "    for a in ngram_range:\n",
        "      for b in min_df:\n",
        "        for c in max_df:\n",
        "          for d in analyzer:\n",
        "            for e in sublinear_tf:\n",
        "              for f in norm:\n",
        "                for g in n:\n",
        "                  cfg = [a, b, c, d, e, f, g]\n",
        "                  models.append(cfg)\n",
        "    return models\n",
        "\n",
        "configs = configs()\n",
        "print(configs)\n",
        "print(len(configs))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(1, 2), 2, 1.0, 'word', True, 'l2', 45]]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XYnU2mv6x6j"
      },
      "source": [
        "**TO BEAT :** [[(1, 2), 2, 1.0, 'word', True, 'l2', 45], 0.7868725868725869, 0.7228915662650602] \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "behnovAJ6kbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c312379f-620e-463d-92b3-52c5ccc7c660"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Define list for result\n",
        "result = []\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "  # Redefine vectorizer\n",
        "  tfidf_vector = TfidfVectorizer(tokenizer=simple_preprocess, \n",
        "                                ngram_range=config[0],\n",
        "                                min_df=config[1],\n",
        "                                 max_df=config[2], \n",
        "                                 analyzer=config[3], \n",
        "                                 sublinear_tf=config[4], \n",
        "                                 norm=config[5], \n",
        "                                 encoding='latin-1')\n",
        "\n",
        "  # Define classifier\n",
        "  classifier = KNeighborsClassifier(n_neighbors=config[6], n_jobs=cores)\n",
        "\n",
        "  # Create pipeline\n",
        "  pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "  # Fit model on training set\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions\n",
        "  y_pred = pipe.predict(X_test)\n",
        "\n",
        "  # Print accuracy on test set\n",
        "  print(\"CONFIG: \", config)\n",
        "  print(f\"TEST ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "  print(\"-----------------------\")\n",
        "\n",
        "  # Append to result\n",
        "  result.append([config, accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)])\n",
        "\n",
        "result.sort(key=lambda x:x[1])\n",
        "print('Top parameters (accuracy) :\\n')\n",
        "print(result[-1], '\\n')\n",
        "result.sort(key=lambda x:x[2])\n",
        "print('Top parameters (F1) :\\n')\n",
        "print(result[-1])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(1, 2), 2, 1.0, 'word', True, 'l2', 45]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7869\n",
            "TEST F1 SCORE:\n",
            "0.7229\n",
            "TEST CONFUSION MATRIX:\n",
            "[[659  88]\n",
            " [188 360]]\n",
            "-----------------------\n",
            "Top parameters (accuracy) :\n",
            "\n",
            "[[(1, 2), 2, 1.0, 'word', True, 'l2', 45], 0.7868725868725869, 0.7228915662650602] \n",
            "\n",
            "Top parameters (F1) :\n",
            "\n",
            "[[(1, 2), 2, 1.0, 'word', True, 'l2', 45], 0.7868725868725869, 0.7228915662650602]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwtfeH_R6iGV"
      },
      "source": [
        "### 6.4.2. KNN CountVector ---------- MAX ACCURACY = 0.7336"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t57yrf16kMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63e3cb3-9361-4995-c49d-bbfedd1106f9"
      },
      "source": [
        "# Create list of configs\n",
        "def configs():\n",
        "\n",
        "    models = list()\n",
        "    \n",
        "    # Define config lists\n",
        "    ngram_range = [(2,3)] #0  [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]\n",
        "    min_df = [1] #1 [0, 1, 2, 3, 4, 5]\n",
        "    max_df = [1.0] #2 [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    analyzer=['char'] #3  ['word', 'char', 'char_wb']\n",
        "\n",
        "    n = [1] #4  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 150, 200]\n",
        "\n",
        "    # Create config instances\n",
        "    for a in ngram_range:\n",
        "      for b in min_df:\n",
        "        for c in max_df:\n",
        "          for d in analyzer:\n",
        "            for e in n:\n",
        "              cfg = [a, b, c, d, e]\n",
        "              models.append(cfg)\n",
        "    return models\n",
        "\n",
        "configs = configs()\n",
        "print(configs, '\\n')\n",
        "print('Number of configs :', len(configs))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(2, 3), 1, 1.0, 'char', 1]] \n",
            "\n",
            "Number of configs : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pmb4L6D7C1R"
      },
      "source": [
        "**TO BEAT :** [[(2, 3), 1, 1.0, 'char', 1], 0.7335907335907336, 0.6820276497695853]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOi1-ReJ6kGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a600f6-a378-462b-f0d5-3d089ba35b3a"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Define list for result\n",
        "result = []\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "  # Redefine vectorizer\n",
        "  tfidf_vector = CountVectorizer(tokenizer=simple_preprocess, \n",
        "                                ngram_range=config[0],\n",
        "                                min_df=config[1],\n",
        "                                max_df=config[2], \n",
        "                                analyzer=config[3], \n",
        "                                encoding='latin-1')\n",
        "\n",
        "  # Define classifier\n",
        "  classifier = KNeighborsClassifier(n_neighbors=config[4], n_jobs=cores)\n",
        "\n",
        "  # Create pipeline\n",
        "  pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "  # Fit model on training set\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions\n",
        "  y_pred = pipe.predict(X_test)\n",
        "\n",
        "  # Print accuracy on test set\n",
        "  print(\"CONFIG: \", config)\n",
        "  print(f\"TEST ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "  print(f\"TEST CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "  print(\"-----------------------\")\n",
        "\n",
        "  # Append to result\n",
        "  result.append([config, accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)])\n",
        "\n",
        "result.sort(key=lambda x:x[1])\n",
        "print('Top parameters (accuracy) :\\n')\n",
        "print(result[-1], '\\n')\n",
        "result.sort(key=lambda x:x[2])\n",
        "print('Top parameters (F1) :\\n')\n",
        "print(result[-1])"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFIG:  [(2, 3), 1, 1.0, 'char', 1]\n",
            "TEST ACCURACY SCORE:\n",
            "0.7336\n",
            "TEST F1 SCORE:\n",
            "0.6820\n",
            "TEST CONFUSION MATRIX:\n",
            "[[580 167]\n",
            " [178 370]]\n",
            "-----------------------\n",
            "Top parameters (accuracy) :\n",
            "\n",
            "[[(2, 3), 1, 1.0, 'char', 1], 0.7335907335907336, 0.6820276497695853] \n",
            "\n",
            "Top parameters (F1) :\n",
            "\n",
            "[[(2, 3), 1, 1.0, 'char', 1], 0.7335907335907336, 0.6820276497695853]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9F3FN8NRc9O"
      },
      "source": [
        "## 6.5. Doc 2 Vec ---------- MAX ACCURACY = 0.8046"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pjGOB_TRiNU",
        "outputId": "44e75e5a-a9bf-411b-f392-922affe427f2"
      },
      "source": [
        "# Tokenize data - same tokenizer function as before\n",
        "%%time\n",
        "sample_tagged = Train.apply(lambda r: TaggedDocument(words=simple_preprocess(r['text']), tags=[r.target]), axis=1)\n",
        "\n",
        "# Train test split - same split as before\n",
        "train_tagged, test_tagged = train_test_split(sample_tagged, test_size=0.2, random_state=SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 309 ms, sys: 5.97 ms, total: 315 ms\n",
            "Wall time: 318 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA63Wdwt4d2c"
      },
      "source": [
        "# Define Doc2Vec and build vocabulary\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=38, negative=3, hs=0, min_count=1, sample=SEED, workers=cores, epoch=250)\n",
        "model_dbow.build_vocab([x for x in train_tagged.values])\n",
        "\n",
        "# Train distributed Bag of Word model\n",
        "model_dbow.train(train_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_iWQs1w4gNw"
      },
      "source": [
        "# Select X and y\n",
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=100)) for doc in sents])\n",
        "    return targets, regressors\n",
        "\n",
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoIln8894i1y",
        "outputId": "4f5c548b-fe78-4706-a2e6-f5e9c9371c49"
      },
      "source": [
        "# Fit model on training set - same algorithm as before\n",
        "logreg = LogisticRegression(C=1.0, max_iter=1000, solver='lbfgs', random_state=SEED)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "print('ACCURACY SCORE :', round(accuracy_score(y_test, y_pred), 4))\n",
        "print('F1 SCORE :', round(f1_score(y_test, y_pred), 4))\n",
        "conf_mat = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY SCORE : 0.7946\n",
            "F1 SCORE : 0.7486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR1M-jG5TgRV"
      },
      "source": [
        "## 6.6. Additionnal features with LogReg --------- MAX ACCURACY = 0.8093\r\n",
        "\r\n",
        "This could be more improved if we had more time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnYnqOhSOm0y"
      },
      "source": [
        "# Create the TFIDF vector\n",
        "tfidf_vector = TfidfVectorizer(ngram_range=(1,1), min_df=3, max_df=1.0, analyzer='word', sublinear_tf=True, norm='l2', tokenizer=simple_preprocess)\n",
        "\n",
        "# TFIDF'isation of texts\n",
        "X_tfidf = pd.DataFrame(tfidf_vector.fit_transform(X).todense())\n",
        "X_fin_tfidf = pd.DataFrame(tfidf_vector.transform(X_fin).todense())\n",
        "\n",
        "# Adding additionnal datas\n",
        "X_add = pd.concat([X_tfidf, Train_add], axis=1)\n",
        "X_fin_add = pd.concat([X_fin_tfidf, Test_add], axis=1)\n",
        "\n",
        "# Splitting X and y :\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_add, y, test_size=0.2, random_state=SEED)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9c4PBZM1ijz",
        "outputId": "2c84a374-3535-46fc-82fd-c2c82a6ecaa9"
      },
      "source": [
        "#Create the classifier:\n",
        "modeladd = LogisticRegression(C=1.45, max_iter=1000, n_jobs=cores, random_state=SEED, solver='lbfgs')\n",
        "\n",
        "modeladd.fit(X_train,y_train)\n",
        "\n",
        "y_pred = modeladd.predict(X_test)\n",
        "\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY SCORE:\n",
            "0.8093\n",
            "F1 SCORE:\n",
            "0.7600\n",
            "CONFUSION MATRIX:\n",
            "[[657  90]\n",
            " [157 391]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER10ZUoUsAPy",
        "outputId": "b0080330-4f08-42f6-b1d7-81d05e1c1170"
      },
      "source": [
        "# Building the final model without splitting (full dataset)\n",
        "modeladd = LogisticRegression(C=0.65, max_iter=1000, n_jobs=-2, random_state=SEED, solver='lbfgs', verbose=10)\n",
        "\n",
        "# Fitting the model\n",
        "modeladd.fit(X_add, y)\n",
        "y_pred = modeladd.predict(X_add)\n",
        "\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y, y_pred):.4f}\")\n",
        "print(f\"F1 SCORE:\\n{f1_score(y, y_pred):.4f}\")\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y, y_pred)}\")\n",
        "\n",
        "import time\n",
        "timestr = time.strftime(\"%d%m-%H%M\")\n",
        "\n",
        "# Predicted labels of X_fin by the final model\n",
        "y_fin = pd.DataFrame()\n",
        "y_pred_fin = modeladd.predict(X_fin_add)\n",
        "y_fin['target']= y_pred_fin\n",
        "\n",
        "# Saving it to CSV with timestamp :\n",
        "y_fin.to_csv('y_' + timestr + '.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=-2)]: Done   1 out of   1 | elapsed:    3.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=-2)]: Done   1 out of   1 | elapsed:    3.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ACCURACY SCORE:\n",
            "0.8575\n",
            "F1 SCORE:\n",
            "0.8183\n",
            "CONFUSION MATRIX:\n",
            "[[3473  228]\n",
            " [ 694 2076]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wwgnDMsAtK1"
      },
      "source": [
        "## 6.7. GridSearch on logreg\n",
        "Problem of GridSearch : Even with verbose = 10, it happened that after long loops (more than 4 hours), the programm crashes, and it's impossible to look back at what was done. That's why we decided to user the config list method. Eventually, we used gridsearch to find the optimal C only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28nBLhNHYTi4",
        "outputId": "b162e4a6-3ed6-4dd5-f0db-7d459d18f3bb"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Create the TFIDF vector\n",
        "tfidf_vector = TfidfVectorizer(ngram_range=(1,1), min_df=3, max_df=1.0, analyzer='word', sublinear_tf=True, norm='l2', encoding='latin-1', tokenizer=simple_preprocess)\n",
        "\n",
        "#Create the classifier:\n",
        "classifier = LogisticRegression(max_iter=1000, n_jobs=cores, random_state=SEED, solver='lbfgs', verbose=10)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)], verbose=10)\n",
        "\n",
        "param_grid = [\n",
        "{'classifier' : [LogisticRegression(max_iter=1000, random_state=SEED)],\n",
        " 'classifier__penalty' : ['l1', 'l2'],\n",
        " 'classifier__C' : np.logspace(-4, 4, 20),\n",
        " 'classifier__solver' : ['liblinear', 'lbfgs']}\n",
        "]\n",
        "\n",
        "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5,verbose=10 , n_jobs=-2, scoring='f1')\n",
        "\n",
        "best_clf = clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "print(best_clf.best_estimator_.get_params()['classifier'])\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=-2)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=-2)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   3 out of   3 | elapsed:    0.5s remaining:    0.0s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:    0.7s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=-2)]: Done   5 out of   5 | elapsed:    0.9s remaining:    0.0s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "[Parallel(n_jobs=-2)]: Done   6 out of   6 | elapsed:    1.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "[Parallel(n_jobs=-2)]: Done   7 out of   7 | elapsed:    1.1s remaining:    0.0s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "[Parallel(n_jobs=-2)]: Done   8 out of   8 | elapsed:    1.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "[Parallel(n_jobs=-2)]: Done   9 out of   9 | elapsed:    1.4s remaining:    0.0s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    1.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   1.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1687s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0001, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1845s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1785s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1205s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1590s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.00026366508987303583, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1362s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1833s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1740s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1535s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0006951927961775605, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1773s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1822s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1360s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1400s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1985s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.0018329807108324356, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1363s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1141s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1332s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1401s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1535s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.004832930238571752, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1606s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1920s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1405s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1752s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1394s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.012742749857031334, classifier__penalty=l2, classifier__solver=lbfgs, score=0.000, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=liblinear, score=0.000, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear, score=0.262, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear, score=0.282, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear, score=0.244, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear, score=0.259, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=liblinear, score=0.287, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs, score=0.234, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1944s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs, score=0.262, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1769s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs, score=0.237, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs, score=0.228, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1768s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.03359818286283781, classifier__penalty=l2, classifier__solver=lbfgs, score=0.260, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear, score=0.274, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear, score=0.330, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear, score=0.270, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear, score=0.244, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=liblinear, score=0.331, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear, score=0.611, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear, score=0.622, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear, score=0.619, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear, score=0.627, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=liblinear, score=0.642, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs, score=0.610, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs, score=0.616, total=   0.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs, score=0.617, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs, score=0.625, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.08858667904100823, classifier__penalty=l2, classifier__solver=lbfgs, score=0.642, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear, score=0.569, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear, score=0.593, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear, score=0.573, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear, score=0.561, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=liblinear, score=0.570, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear, score=0.703, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear, score=0.695, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear, score=0.704, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear, score=0.737, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=liblinear, score=0.723, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs, score=0.703, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs, score=0.694, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Batch computation too fast (0.1796s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs, score=0.705, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs, score=0.733, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.23357214690901212, classifier__penalty=l2, classifier__solver=lbfgs, score=0.723, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear, score=0.680, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear, score=0.683, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear, score=0.694, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear, score=0.706, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=liblinear, score=0.698, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear, score=0.729, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear, score=0.721, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear, score=0.720, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear, score=0.756, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=liblinear, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs, score=0.730, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs, score=0.721, total=   0.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs, score=0.720, total=   0.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs, score=0.756, total=   0.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=0.615848211066026, classifier__penalty=l2, classifier__solver=lbfgs, score=0.746, total=   0.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear, score=0.723, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear, score=0.718, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear, score=0.711, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear, score=0.741, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=liblinear, score=0.733, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear, score=0.734, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear, score=0.737, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear, score=0.760, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=liblinear, score=0.753, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs, score=0.735, total=   0.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.5s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs, score=0.737, total=   0.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.8s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs, score=0.724, total=   1.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs, score=0.761, total=   0.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1.623776739188721, classifier__penalty=l2, classifier__solver=lbfgs, score=0.753, total=   0.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear, score=0.717, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear, score=0.719, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear, score=0.718, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear, score=0.738, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=liblinear, score=0.739, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear, score=0.733, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear, score=0.732, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear, score=0.723, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear, score=0.744, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=liblinear, score=0.743, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs, score=0.733, total=   0.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.8s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs, score=0.732, total=   1.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs, score=0.723, total=   0.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs, score=0.744, total=   1.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    1.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=4.281332398719396, classifier__penalty=l2, classifier__solver=lbfgs, score=0.741, total=   1.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear, score=0.704, total=   0.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear, score=0.708, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear, score=0.705, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear, score=0.710, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=liblinear, score=0.720, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear, score=0.724, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear, score=0.716, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear, score=0.715, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear, score=0.727, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=liblinear, score=0.731, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    2.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs, score=0.724, total=   2.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    1.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.5s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs, score=0.716, total=   1.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    2.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    2.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs, score=0.715, total=   2.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    1.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs, score=0.727, total=   2.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    1.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.5s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=11.288378916846883, classifier__penalty=l2, classifier__solver=lbfgs, score=0.731, total=   1.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear, score=0.688, total=   0.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear, score=0.685, total=   0.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear, score=0.689, total=   0.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear, score=0.706, total=   0.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=liblinear, score=0.707, total=   0.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear, score=0.719, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear, score=0.719, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear, score=0.717, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear, score=0.729, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=liblinear, score=0.732, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    2.1s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    2.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs, score=0.719, total=   2.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    4.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    4.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs, score=0.719, total=   4.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    2.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.5s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs, score=0.717, total=   2.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    4.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    4.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs, score=0.729, total=   4.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    3.7s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    3.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   3.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=29.763514416313132, classifier__penalty=l2, classifier__solver=lbfgs, score=0.730, total=   4.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear, score=0.685, total=   0.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear, score=0.676, total=   0.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.5s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear, score=0.680, total=   0.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear, score=0.688, total=   1.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=liblinear, score=0.699, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear, score=0.709, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear, score=0.700, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear, score=0.722, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear, score=0.718, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=liblinear, score=0.724, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    5.5s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    5.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   5.5s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs, score=0.709, total=   5.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   10.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   10.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  10.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs, score=0.700, total=  10.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    4.0s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    4.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs, score=0.722, total=   4.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    5.0s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    5.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   5.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs, score=0.718, total=   5.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    6.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    6.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   6.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=78.47599703514607, classifier__penalty=l2, classifier__solver=lbfgs, score=0.725, total=   6.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear, score=0.666, total=   1.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.9s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear, score=0.674, total=   1.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear, score=0.672, total=   1.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear, score=0.691, total=   0.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.5s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=liblinear, score=0.704, total=   0.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear, score=0.695, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear, score=0.694, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear, score=0.708, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear, score=0.709, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=liblinear, score=0.704, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   10.0s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   10.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  10.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs, score=0.695, total=  10.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   10.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   10.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  10.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs, score=0.694, total=  10.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    4.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    4.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs, score=0.708, total=   4.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   11.8s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   11.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  11.8s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs, score=0.710, total=  12.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   10.9s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   10.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  10.9s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=206.913808111479, classifier__penalty=l2, classifier__solver=lbfgs, score=0.704, total=  11.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear, score=0.665, total=   1.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear, score=0.672, total=   1.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear, score=0.668, total=   1.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear, score=0.682, total=   1.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=liblinear, score=0.696, total=   0.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear, score=0.687, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear, score=0.687, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear, score=0.696, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear, score=0.707, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=liblinear, score=0.699, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   11.2s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   11.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  11.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs, score=0.687, total=  11.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   13.1s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   13.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  13.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs, score=0.686, total=  13.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   11.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   11.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  11.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs, score=0.696, total=  11.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   19.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   19.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  19.5s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs, score=0.707, total=  19.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    9.7s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    9.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   9.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=545.5594781168514, classifier__penalty=l2, classifier__solver=lbfgs, score=0.699, total=  10.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear, score=0.661, total=   1.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.8s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear, score=0.666, total=   2.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.6s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear, score=0.655, total=   1.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear, score=0.676, total=   1.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=liblinear, score=0.695, total=   2.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear, score=0.687, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear, score=0.684, total=   0.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear, score=0.687, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear, score=0.704, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=liblinear, score=0.692, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   21.4s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   21.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  21.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs, score=0.687, total=  21.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   21.6s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   21.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  21.6s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs, score=0.684, total=  22.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   20.7s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   20.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  20.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs, score=0.687, total=  21.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   27.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   27.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  27.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs, score=0.704, total=  27.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   18.3s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   18.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  18.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=1438.44988828766, classifier__penalty=l2, classifier__solver=lbfgs, score=0.692, total=  18.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear, score=0.645, total=   2.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear, score=0.654, total=   1.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear, score=0.658, total=   2.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear, score=0.675, total=   1.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.9s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=liblinear, score=0.687, total=   1.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear, score=0.683, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear, score=0.676, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear, score=0.681, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear, score=0.703, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=liblinear, score=0.694, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   32.8s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   32.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  32.8s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs, score=0.683, total=  33.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   32.9s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   32.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  32.9s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs, score=0.676, total=  33.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   31.0s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   31.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  31.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs, score=0.681, total=  31.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   34.9s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   34.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  34.9s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs, score=0.701, total=  35.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   39.8s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   39.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  39.8s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=3792.690190732246, classifier__penalty=l2, classifier__solver=lbfgs, score=0.696, total=  40.1s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.4s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear, score=0.640, total=   1.7s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.9s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear, score=0.653, total=   2.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear, score=0.647, total=   1.4s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear, score=0.678, total=   2.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=liblinear, score=0.682, total=   1.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l1, classifier__solver=lbfgs, score=nan, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear][Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear, score=0.673, total=   0.5s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear, score=0.676, total=   0.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear, score=0.672, total=   0.6s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear, score=0.687, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=liblinear, score=0.693, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   41.9s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   41.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  42.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs, score=0.673, total=  42.2s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   43.5s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   43.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  43.5s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs, score=0.676, total=  43.8s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   42.6s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   42.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  42.6s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs, score=0.670, total=  43.0s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   35.7s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   35.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  35.7s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs, score=0.687, total=  35.9s\n",
            "[CV] classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   49.1s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:   49.1s finished\n",
            "[Parallel(n_jobs=-2)]: Done 400 out of 400 | elapsed: 13.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  49.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='auto', n_jobs=2, penalty='l2', random_state=72,\n",
            "                   solver='lbfgs', tol=0.0001, verbose=10, warm_start=False), classifier__C=10000.0, classifier__penalty=l2, classifier__solver=lbfgs, score=0.692, total=  49.5s\n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.7s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td-VbNH3Cgyl"
      },
      "source": [
        "\n",
        "\n",
        "> LogisticRegression(C=0.615848211066026,class_weight=None, dual=False,\n",
        "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
        "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
        "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
        "                   warm_start=False)\n",
        "\n",
        "ACCURACY SCORE:\n",
        "\n",
        "\n",
        "F1 SCORE:\n",
        "0.7540\n",
        "\n",
        "CONFUSION MATRIX:\n",
        "\n",
        "[[667  80]\n",
        "\n",
        "[168 380]]\n",
        "\n",
        "---\n",
        "\n",
        "> LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
        "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
        "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
        "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=10,\n",
        "                   warm_start=False)\n",
        "\n",
        "ACCURACY SCORE:\n",
        "\n",
        "\n",
        "F1 SCORE:\n",
        "0.7646\n",
        "\n",
        "CONFUSION MATRIX:\n",
        "\n",
        "[[652  95]\n",
        " \n",
        " [150 398]]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeLDMiI-G27X",
        "outputId": "cb678b27-a14a-47f1-df4f-4ee24eb1ad15"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "    \n",
        "# Create the TFIDF vector\n",
        "tfidf_vector = TfidfVectorizer(ngram_range=(1,1), min_df=3, max_df=1.0, analyzer='word', sublinear_tf=True, norm='l2', encoding='latin-1', tokenizer=simple_preprocess)\n",
        "\n",
        "#Create the classifier:\n",
        "classifier = LogisticRegression(C=1.623776739188721, max_iter=1000, n_jobs=cores, random_state=SEED, solver='lbfgs')\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)], verbose=10)\n",
        "\n",
        "param_grid = [\n",
        "{'vectorizer' : [TfidfVectorizer(tokenizer=simple_preprocess)],\n",
        " 'vectorizer__smooth_idf' : [True, False],\n",
        " 'vectorizer__analyzer' : ['word'],\n",
        " 'vectorizer__min_df' : [0, 1, 2, 3, 4, 5],\n",
        " 'vectorizer__max_df' : [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        " 'vectorizer__use_idf' : [True, False],\n",
        " 'classifier' : [LogisticRegression(C=1.623776739188721, max_iter=1000, n_jobs=cores, random_state=SEED, solver='lbfgs')]}\n",
        "]\n",
        "\n",
        "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=10, n_jobs=-2, scoring='f1')\n",
        "\n",
        "best_clf = clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "print(best_clf.best_estimator_.get_params()['vectorizer'])\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.731, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.741, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   2 out of   2 | elapsed:    0.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.731, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   3 out of   3 | elapsed:    0.8s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.765, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:    1.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.749, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   5 out of   5 | elapsed:    1.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.734, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   6 out of   6 | elapsed:    1.6s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.724, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   7 out of   7 | elapsed:    1.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.726, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   8 out of   8 | elapsed:    2.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.759, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.5, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done   9 out of   9 | elapsed:    2.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.749, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.746, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.734, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.741, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.726, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.759, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.749, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.734, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.729, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.725, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.749, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.746, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.736, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.741, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.731, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.757, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.748, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.728, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.723, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.720, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.758, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.736, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.742, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.731, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.758, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.748, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.728, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.723, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.720, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.758, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.736, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.737, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.752, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.741, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.729, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.723, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.713, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.756, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.748, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.738, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.738, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.753, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.742, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.729, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.723, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.713, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.756, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.748, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.737, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.738, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.727, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.756, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.726, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.712, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.752, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.742, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.737, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.738, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.726, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.756, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.726, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.712, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.752, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=0.9, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.742, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.731, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.727, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.762, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.752, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.734, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.729, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.719, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.757, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.742, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.732, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.745, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.729, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.760, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.753, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.734, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.729, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.719, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.757, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=0, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.742, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.731, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.746, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.727, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.762, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.752, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.734, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.729, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.719, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.757, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.742, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.732, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.745, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.729, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.760, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.753, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.734, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.729, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.719, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.757, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=1, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.742, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.734, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.741, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.726, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.757, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.749, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.734, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.729, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.749, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.746, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.734, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.741, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.726, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.759, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.749, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.734, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.729, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.725, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.749, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=2, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.746, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.736, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.741, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.731, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.757, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.748, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.728, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.723, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.720, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.758, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.736, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.742, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.731, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.758, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.748, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.728, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.723, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.720, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.758, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=3, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.736, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.737, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.752, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.741, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.729, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.723, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.713, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.756, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.748, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.738, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.738, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.753, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.742, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.729, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.723, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.713, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.756, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=4, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.748, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.737, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.738, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.727, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.756, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=True, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.726, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.725, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.712, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.752, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=True, vectorizer__use_idf=False, score=0.742, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.737, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.738, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.726, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.756, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.0s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=True, score=0.746, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.726, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.725, total=   0.3s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.712, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.752, total=   0.2s\n",
            "[CV] classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False \n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.1s\n",
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "[CV]  classifier=LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), vectorizer=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None), vectorizer__analyzer=word, vectorizer__max_df=1.0, vectorizer__min_df=5, vectorizer__smooth_idf=False, vectorizer__use_idf=False, score=0.742, total=   0.2s\n",
            "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done 720 out of 720 | elapsed:  3.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
            "LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='auto', n_jobs=2, penalty='l2',\n",
            "                   random_state=72, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "ACCURACY SCORE:\n",
            "0.8054\n",
            "F1 SCORE:\n",
            "0.7586\n",
            "CONFUSION MATRIX:\n",
            "[[647 100]\n",
            " [152 396]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUm1kBfOZyK_",
        "outputId": "f0fb5965-fb42-4688-c00d-7464e33e37c7"
      },
      "source": [
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "print(best_clf.best_estimator_.get_params()['vectorizer'])\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.6, max_features=None,\n",
            "                min_df=0, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=False, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
            "                use_idf=True, vocabulary=None)\n",
            "ACCURACY SCORE:\n",
            "0.8054\n",
            "F1 SCORE:\n",
            "0.7586\n",
            "CONFUSION MATRIX:\n",
            "[[647 100]\n",
            " [152 396]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf1JTJ8wXzUJ"
      },
      "source": [
        "> TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
        "                input='content', lowercase=True, max_df=0.6, max_features=None,\n",
        "                min_df=0, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
        "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
        "                sublinear_tf=True, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
        "                use_idf=True, vocabulary=None)\n",
        "\n",
        "ACCURACY SCORE:\n",
        "0.8093\n",
        "\n",
        "F1 SCORE:\n",
        "0.7627\n",
        "\n",
        "CONFUSION MATRIX:\n",
        "\n",
        "[[651  96]\n",
        "\n",
        " [151 397]]\n",
        "\n",
        "---\n",
        "\n",
        "> TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
        "                input='content', lowercase=True, max_df=0.6, max_features=None,\n",
        "                min_df=0, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
        "                smooth_idf=False, stop_words=None, strip_accents=None,\n",
        "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "                tokenizer=<function simple_preprocess at 0x7f5387240158>,\n",
        "                use_idf=True, vocabulary=None)\n",
        "\n",
        "ACCURACY SCORE:\n",
        "0.8054\n",
        "\n",
        "F1 SCORE:\n",
        "0.7586\n",
        "\n",
        "CONFUSION MATRIX:\n",
        "\n",
        "[[647 100]\n",
        "\n",
        " [152 396]]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYSyxbn1-AcH"
      },
      "source": [
        "def modelisationknn(features, target, split=True):\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=SEED)\n",
        "\n",
        "    if not split:\n",
        "      print('No split')\n",
        "      X_train = features\n",
        "      y_train = target\n",
        "      X_test = features\n",
        "      y_test = target\n",
        "\n",
        "      \n",
        "    # Create the TFIDF vector\n",
        "    tfidf_vector = TfidfVectorizer(ngram_range=(1,1), min_df=3, max_df=1.0, analyzer='word', sublinear_tf=True, norm='l2', encoding='latin-1', tokenizer=simple_preprocess)\n",
        "\n",
        "    #Create the classifier:\n",
        "    classifier = KNeighborsClassifier(n_neighbors=7, n_jobs=cores)\n",
        "\n",
        "    # Create pipeline\n",
        "    pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "    # Fit model on training set\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "        \n",
        "    return pipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3q17JBZJ0m9",
        "outputId": "c9625eec-e6d0-42b6-977b-99e752888bfd"
      },
      "source": [
        "modelknn = modelisationknn(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY SCORE:\n",
            "0.7637\n",
            "CONFUSION MATRIX:\n",
            "[[616 131]\n",
            " [175 373]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcpPMmF7jDYb"
      },
      "source": [
        "# 7. Final Model Building\n",
        "\n",
        "Building of the final model with the best parameters we found in the optimization step. Then manually try to increase a little bit accuracy and/or F1 score ( C=1.3 +0.01, ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lEuqFmZFgnj"
      },
      "source": [
        "def modelisationlogreg(features, target, split=True):\n",
        "\n",
        "    # Splitting X and y :\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=SEED)\n",
        "\n",
        "    # NO SPLIT for the final model building :\n",
        "    if not split:\n",
        "      print('NO SPLIT')\n",
        "      X_train = features\n",
        "      y_train = target\n",
        "      X_test = features\n",
        "      y_test = target\n",
        "\n",
        "      \n",
        "    # Create the TFIDF vector\n",
        "    tfidf_vector = TfidfVectorizer(ngram_range=(1,1), min_df=3, max_df=1.0, analyzer='word', sublinear_tf=True, norm='l2', tokenizer=simple_preprocess)\n",
        "\n",
        "    #Create the classifier:\n",
        "    classifier = LogisticRegression(C=1.21, max_iter=1000, n_jobs=cores, random_state=SEED, solver='lbfgs')\n",
        "\n",
        "    # Create pipeline\n",
        "    pipe = Pipeline([('vectorizer', tfidf_vector), ('classifier', classifier)])\n",
        "\n",
        "    # Fit model on training set\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"F1 SCORE:\\n{f1_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "        \n",
        "    return pipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drxjAhQaZUiD",
        "outputId": "dbca9906-947a-45b0-87fd-6d43e9a92297"
      },
      "source": [
        "# Building the model with splitting (validation)\n",
        "%%time\n",
        "model = modelisationlogreg(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY SCORE:\n",
            "0.8147\n",
            "F1 SCORE:\n",
            "0.7679\n",
            "CONFUSION MATRIX:\n",
            "[[658  89]\n",
            " [151 397]]\n",
            "CPU times: user 245 ms, sys: 40 ms, total: 285 ms\n",
            "Wall time: 1.26 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fQe_kqql2si",
        "outputId": "993ba4f1-defe-4342-8b0b-e8c61381d397"
      },
      "source": [
        "# Building the final model without splitting (full dataset)\n",
        "model = modelisationlogreg(X, y, False)\n",
        "\n",
        "import time\n",
        "timestr = time.strftime(\"%d%m-%H%M\")\n",
        "\n",
        "# Predicted labels of X_fin by the final model\n",
        "y_fin = pd.DataFrame()\n",
        "y_pred_fin = model.predict(X_fin)\n",
        "y_fin['target']= y_pred_fin\n",
        "\n",
        "# Saving it to CSV with timestamp :\n",
        "y_fin.to_csv('y_' + timestr + '.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NO SPLIT\n",
            "ACCURACY SCORE:\n",
            "0.8669\n",
            "F1 SCORE:\n",
            "0.8323\n",
            "CONFUSION MATRIX:\n",
            "[[3473  228]\n",
            " [ 633 2137]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1fqNfpbfty1"
      },
      "source": [
        "The accuracy of the final model on the prediction of the test datas is **0.832**."
      ]
    }
  ]
}